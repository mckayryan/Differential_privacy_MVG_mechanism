{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "import pandas\n",
    "import numpy\n",
    "import glob\n",
    "import re\n",
    "\n",
    "# local imports\n",
    "from differential_privacy_parameters import get_query_point_sensitivity\n",
    "from differential_privacy_parameters import get_query_row_sensitivity\n",
    "from differential_privacy_parameters import get_query_gamma\n",
    "\n",
    "from differential_privacy_mechanisms import gaussian_mechanism_matrix_sample\n",
    "from differential_privacy_mechanisms import matrixvariate_gaussian_mechanism_sample\n",
    "from differential_privacy_mechanisms import MVGMechanism\n",
    "\n",
    "from model_evaluation import test_train_split\n",
    "from model_evaluation import principle_component_RSS\n",
    "from model_evaluation import root_mean_squared_error\n",
    "from model_evaluation import record_result\n",
    "\n",
    "from preprocessing import centered_sample_covariance_matrix\n",
    "from preprocessing import scale_data\n",
    "\n",
    "from models import seq_nn_single_evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_dp_mechanism_krr_cv(data,\n",
    "                                 samples,\n",
    "                                 krr_kernel,\n",
    "                                 scoring_function,\n",
    "                                 privacy_mechanism_sampler,\n",
    "                                 mechanism_args):\n",
    "    '''\n",
    "    evaluate_dp_mechanism_krr_cv\n",
    "    \n",
    "    Evaluates differentially private mechanism by fitting a Kernel Ridge Regression (KRR) model\n",
    "    returning its cross validation score (using the supplied scoring function)\n",
    "    \n",
    "        - Generates differentially private sample from supplied privacy mechanism\n",
    "        - Tune for lowest score KRR hyperparameters\n",
    "        - Return cross validation scores for model fit with tuned hyperparameters\n",
    "    '''\n",
    "    from sklearn.kernel_ridge import KernelRidge\n",
    "    \n",
    "    result = numpy.empty(e1_samples)\n",
    "    \n",
    "    test_X = data[:,:-1]\n",
    "    test_y = data[:,-1]\n",
    "    \n",
    "    for i in xrange(e1_samples):\n",
    "    \n",
    "        mechanism_sample = privacy_mechanism_sampler(\n",
    "            data=data,\n",
    "            **mechanism_args)\n",
    "        \n",
    "        parameters = krr_private_param_rand_search(\n",
    "            train_X=mechanism_sample['X'],\n",
    "            train_y=mechanism_sample['y'],\n",
    "            test_X=test_X,\n",
    "            test_y=test_y,\n",
    "            rand_iters=100,\n",
    "            krr_kernel=krr_kernel,\n",
    "            scoring_function=scoring_function)\n",
    "        \n",
    "        model = KernelRidge(kernel=krr_kernel, \n",
    "                            alpha=parameters['best_alpha'], \n",
    "                            gamma=parameters['best_gamma'])\n",
    "\n",
    "        return krr_private_cross_validate(\n",
    "            train_X=mechanism_sample['X'],\n",
    "            train_y=mechanism_sample['y'],\n",
    "            test_X=test_X,\n",
    "            test_y=test_y,\n",
    "            krr=model,\n",
    "            score_summary_function=numpy.mean,\n",
    "            scoring_function=scoring_function)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  Data Processing and Setup\n",
    "\n",
    "Import and concatonate all data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading...\tdata\\California_100000_20190227\n",
      "Loading...\tdata\\Florida_100000_20190227\n",
      "Loading...\tdata\\Georgia_100000_20190227\n",
      "Loading...\tdata\\Illinois_100000_20190227\n",
      "Loading...\tdata\\New York_100000_20190227\n",
      "Loading...\tdata\\Ohio_100000_20190227\n",
      "Loading...\tdata\\Pennsylvania_100000_20190227\n",
      "Loading...\tdata\\Texas_100000_20190227\n"
     ]
    }
   ],
   "source": [
    "target_dir = 'data/'\n",
    "\n",
    "data_load = None\n",
    "for file_name in glob.glob(target_dir + '*'):\n",
    "    if not(re.search(r'\\.data$',file_name)):\n",
    "        print('Loading...\\t' + file_name)\n",
    "        if data_load is None:\n",
    "            data_load = pandas.read_pickle(file_name)\n",
    "        else:\n",
    "            data_load = pandas.concat([data_load,\n",
    "                                       pandas.read_pickle(file_name)], \n",
    "                                      sort=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_load.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Scale data and establish evaluation parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "evaluation_features = [\n",
    "    'bmi',\n",
    "    'diastolic_blood_pressure',\n",
    "    'systolic_blood_pressure',\n",
    "    'glucose',\n",
    "    'hdl_cholesterol',\n",
    "    'ldl_cholesterol',\n",
    "    'total_cholesterol',\n",
    "    'triglycerides',\n",
    "    'age',\n",
    "    'framingham'    \n",
    "]\n",
    "\n",
    "data_feature_bounds = {\n",
    "    'bmi':(0,400),\n",
    "    'diastolic_blood_pressure':(60,140),\n",
    "    'systolic_blood_pressure':(90,250),\n",
    "    'glucose':(0,2000),\n",
    "    'hdl_cholesterol':(0,1500),\n",
    "    'ldl_cholesterol':(0,2000),\n",
    "    'total_cholesterol':(0,2100),\n",
    "    'triglycerides':(0,3000),\n",
    "    'age':(0,120),\n",
    "    'framingham':(-10,37)\n",
    "}\n",
    "target_feature_bounds = (0,1)\n",
    "\n",
    "# Setup for estimation of framingham score\n",
    "response = ['framingham']\n",
    "predictors = [ f for f in evaluation_features if f not in response]\n",
    "\n",
    "results_columns = [\n",
    "    'mechanism', \n",
    "    'query', \n",
    "    'sample size',\n",
    "    'iteration', \n",
    "    'metric', \n",
    "    'result', \n",
    "    'mechanism runtime (s)', \n",
    "    'total runtime (s)'\n",
    "]\n",
    "result_pickle_location = 'results/'\n",
    "\n",
    "# Scale data 'data_feature_bounds' -> 'target_feature_bounds'\n",
    "data_scaled = scale_data(data_load[evaluation_features].dropna(),\n",
    "                         target_bounds=target_feature_bounds,\n",
    "                         data_bounds=data_feature_bounds)\n",
    "\n",
    "# Sample data if needed\n",
    "sample_size = 50000\n",
    "evaluation_samples = 20\n",
    "\n",
    "if isinstance(sample_size, int) and sample_size < len(data_scaled):\n",
    "    data = data_scaled.sample(sample_size)\n",
    "else:\n",
    "    data = data_scaled"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Differential Privacy parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "epsilon = 1.0 \n",
    "# 1 / number of observations\n",
    "delta = pow(data.shape[0], -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import kernel_ridge_regression\n",
    "from kernel_ridge_regression import kernel_ridge_param_search\n",
    "from kernel_ridge_regression import kernel_ridge_cv\n",
    "from kernel_ridge_regression import krr_private_param_rand_search\n",
    "from kernel_ridge_regression import krr_private_cross_validate\n",
    "\n",
    "\n",
    "import sklearn.metrics\n",
    "import sklearn.preprocessing\n",
    "\n",
    "import sklearn.model_selection\n",
    "from sklearn.model_selection import learning_curve\n",
    "from sklearn.model_selection import cross_val_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def root_mean_squared_error(y_true, y_pred):\n",
    "    '''\n",
    "    Computation of evaluation metric RMSE\n",
    "    '''\n",
    "    return pow(sklearn.metrics.mean_squared_error(y_true, y_pred)/ len(y_true), 0.5)\n",
    "\n",
    "\n",
    "def rmse_scorer():\n",
    "    '''\n",
    "    Wrapper function to create sklearn 'scorer' for RMSE metric\n",
    "    '''\n",
    "    return sklearn.metrics.make_scorer(root_mean_squared_error, \n",
    "                                       greater_is_better=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'baseline_identity_50000_framingham_krr_2_20190301'"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n_obs, n_features = data.shape\n",
    "\n",
    "evaluation_test_split = 0.1\n",
    "evalaution_test_size = int(n_obs*evaluation_test_split)\n",
    "evaluation_train_size = n_obs - evalaution_test_size\n",
    "\n",
    "evaluation_samples = 2\n",
    "\n",
    "# Model parameters\n",
    "model_params = dict(epochs=5, batch_size=128, verbose=0) \n",
    "\n",
    "# labelling result values for mechanism\n",
    "mechanism = 'baseline'\n",
    "query_type = 'identity'\n",
    "metric = 'rmse'\n",
    "\n",
    "result_pickle_name = '_'.join([mechanism, \n",
    "                               query_type,\n",
    "                               str(sample_size),\n",
    "                               ''.join(response), \n",
    "                               'krr',\n",
    "                               str(evaluation_samples),\n",
    "                               datetime.date.today().strftime(\"%Y%m%d\")])\n",
    "\n",
    "data_Xy = dict(X=data[predictors].values, y=data[response].values)\n",
    "\n",
    "result_pickle_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "krr_params = kernel_ridge_param_search(\n",
    "    scoring=rmse_scorer(),\n",
    "    kernel='rbf', \n",
    "    cv_folds=5,\n",
    "    random=True,\n",
    "    random_iters=10,\n",
    "    **data_Xy)\n",
    "\n",
    "print(krr_params)\n",
    "\n",
    "# Combine params and data to pass as single dict\n",
    "krr_params.update(data_Xy)\n",
    "\n",
    "krr_result = kernel_ridge_cv(\n",
    "    cv_folds=5,\n",
    "    kernel='rbf', \n",
    "    scoring=rmse_scorer(),\n",
    "    **krr_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = record_result(results=result, \n",
    "                           column_names=results_columns, \n",
    "                           new_data=[[mechanism, \n",
    "                                      query_type, \n",
    "                                      sample_size,\n",
    "                                      1,\n",
    "                                      metric, \n",
    "                                      krr_result, \n",
    "                                      0,\n",
    "                                      0\n",
    "                                     ]])\n",
    "\n",
    "result.to_pickle(result_pickle_location + result_pickle_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# labelling result values for mechanism\n",
    "mechanism = 'gaussian'\n",
    "query_type = 'identity'\n",
    "metric = 'rmse'\n",
    "\n",
    "result_pickle_name = '_'.join([mechanism, \n",
    "                               query_type, \n",
    "                               str(sample_size),\n",
    "                               ''.join(response),\n",
    "                               'krr',\n",
    "                               str(evaluation_samples), \n",
    "                               datetime.date.today().strftime(\"%Y%m%d\")])\n",
    "\n",
    "print(result_pickle_name)\n",
    "\n",
    "model_params = dict(epochs=4, batch_size=128, verbose=0)\n",
    "\n",
    "epsilon = 1.0 \n",
    "# 1 / number of observations\n",
    "delta = pow(evaluation_train_size, -1)\n",
    "\n",
    "sensitivity = get_query_point_sensitivity(query_type='identity',\n",
    "                                          query_scale=target_feature_bounds,\n",
    "                                          query_shape=(evaluation_train_size, n_features))\n",
    "\n",
    "print(sensitivity, epsilon, delta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = None\n",
    "\n",
    "for i in range(3):\n",
    "    \n",
    "    start_clock = datetime.datetime.now()\n",
    "\n",
    "# train_ind, test_ind = \\\n",
    "#         test_train_split(len(data),\n",
    "#                          evaluation_test_split)\n",
    "\n",
    "    sample = gaussian_mechanism_matrix_sample(\n",
    "        data=data,\n",
    "        epsilon=epsilon,\n",
    "        delta=delta,\n",
    "        sensitivity=sensitivity,\n",
    "        symmetric=False,\n",
    "        verbose=False)\n",
    "\n",
    "    end_sample_clock = datetime.datetime.now() \n",
    "\n",
    "    krr_params = krr_private_param_rand_search(\n",
    "        train_X=sample[predictors].values,\n",
    "        train_y=sample[response].values,\n",
    "        test_X=data[predictors].values,\n",
    "        test_y=data[response].values,\n",
    "        scoring_function=root_mean_squared_error,\n",
    "        krr_kernel='rbf', \n",
    "        rand_iters=10)\n",
    "\n",
    "#     print(krr_params)\n",
    "\n",
    "    # Combine params and data to pass as single dict    \n",
    "    model = KernelRidge(kernel='rbf', \n",
    "                        alpha=krr_params['best_alpha'], \n",
    "                        gamma=krr_params['best_gamma'])\n",
    "\n",
    "    metric_result = krr_private_cross_validate(\n",
    "        train_X=sample[predictors].values,\n",
    "        train_y=sample[response].values,\n",
    "        test_X=data[predictors].values,\n",
    "        test_y=data[response].values,\n",
    "        krr=model,\n",
    "        cv_folds=5, \n",
    "        scoring_function=root_mean_squared_error,\n",
    "        score_summary_function=numpy.mean)\n",
    "\n",
    "    end_loop_clock = datetime.datetime.now() \n",
    "\n",
    "\n",
    "    result = record_result(results=result, \n",
    "                           column_names=results_columns, \n",
    "                           new_data=[[mechanism, \n",
    "                                      query_type, \n",
    "                                      sample_size,\n",
    "                                      i+1,\n",
    "                                      metric, \n",
    "                                      metric_result, \n",
    "                                      (end_sample_clock - start_clock).total_seconds(),\n",
    "                                      (end_loop_clock - start_clock).total_seconds()\n",
    "                                     ]])\n",
    "     print(result.iloc[i], start_clock, end_loop_clock)\n",
    "\n",
    "result.to_pickle(result_pickle_location + result_pickle_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    [0.003661622475886985, 0.003613400244978182, 0...\n",
       "0    [0.0037581649038623973, 0.0037162187113041396,...\n",
       "0    [0.0036988310175313155, 0.003752336362770187, ...\n",
       "Name: result, dtype: object"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result.to_pickle(result_pickle_location + result_pickle_name)\n",
    "result.result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation of Sample Covariance Differential Privacy Methods\n",
    "\n",
    "## Gaussian Mechanism with symmetric and identity sampling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Symmetric Gaussian Mechanism\n",
    "\n",
    "Evaluation setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# labelling result values for mechanism\n",
    "mechanism = 'gaussian'\n",
    "query_type = 'covariance'\n",
    "metric = 'principle component RSS'\n",
    "\n",
    "result_pickle_location = 'results/'\n",
    "result_pickle_name = '_'.join([mechanism, \n",
    "                               query_type, \n",
    "                               str(evaluation_samples), \n",
    "                               str(sample_size),\n",
    "                               datetime.date.today().strftime(\"%Y%m%d\")])\n",
    "\n",
    "print(result_pickle_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Differential Privacy parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sensitivity = get_query_point_sensitivity(query_type='covariance',\n",
    "                                          query_scale=target_feature_bounds,\n",
    "                                          query_shape=data.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evaluation of symmetric matrix gaussian mechanism sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = centered_sample_covariance_matrix(X=data)\n",
    "\n",
    "result = None\n",
    "sample = dict()\n",
    "\n",
    "for i in range(evaluation_samples): \n",
    "    # Sample mechanism\n",
    "    start_clock = datetime.datetime.now()\n",
    "    # Add symmetric iid noise\n",
    "    sample[i] = gaussian_mechanism_matrix_sample(\n",
    "                data=query,\n",
    "                epsilon=epsilon,\n",
    "                delta=delta,\n",
    "                sensitivity=sensitivity,\n",
    "                symmetric=True,\n",
    "                verbose=False)\n",
    "    end_sample_clock = datetime.datetime.now() \n",
    "\n",
    "    result = record_result(results=result, \n",
    "                           column_names=results_columns, \n",
    "                           new_data=[[mechanism, \n",
    "                                      query_type, \n",
    "                                      sample_size,\n",
    "                                      i+1,\n",
    "                                      metric, \n",
    "                                      principle_component_RSS(true=query, pred=sample[i]), \n",
    "                                      (end_sample_clock - start_clock).total_seconds(),\n",
    "                                      (end_sample_clock - start_clock).total_seconds()\n",
    "                                     ]])\n",
    "    \n",
    "result.to_pickle(result_pickle_location + result_pickle_name)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Identity Gaussian Mechanism\n",
    "\n",
    "Evaluation setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# labelling result values for mechanism\n",
    "mechanism = 'gaussian'\n",
    "query_type = 'itentity'\n",
    "metric = 'principle component RSS'\n",
    "\n",
    "result_pickle_name = '_'.join([mechanism, \n",
    "                               query_type, \n",
    "                               str(sample_size),\n",
    "                               str(evaluation_samples), \n",
    "                               datetime.date.today().strftime(\"%Y%m%d\")])\n",
    "\n",
    "print(result_pickle_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Differential Privacy parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sensitivity = get_query_row_sensitivity(query_type='identity',\n",
    "                                        query_scale=target_feature_bounds,\n",
    "                                        query_shape=data.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evaluation of identity query guassian mechanism sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = centered_sample_covariance_matrix(X=data)\n",
    "\n",
    "result = None\n",
    "\n",
    "for i in range(evaluation_samples): \n",
    "    # Sample mechanism\n",
    "    start_clock = datetime.datetime.now()\n",
    "    # Add symmetric iid noise\n",
    "    sample = gaussian_mechanism_matrix_sample(\n",
    "                data=query,\n",
    "                epsilon=epsilon,\n",
    "                delta=delta,\n",
    "                sensitivity=sensitivity,\n",
    "                symmetric=False,\n",
    "                verbose=False)\n",
    "    end_sample_clock = datetime.datetime.now() \n",
    "\n",
    "    sample_cov = centered_sample_covariance_matrix(X=sample)\n",
    "    end_loop_clock = datetime.datetime.now() \n",
    "    \n",
    "    result = record_result(results=result, \n",
    "                           column_names=results_columns, \n",
    "                           new_data=[[mechanism, \n",
    "                                      query_type, \n",
    "                                      sample_size,\n",
    "                                      i+1,\n",
    "                                      metric, \n",
    "                                      principle_component_RSS(true=query, pred=sample_cov), \n",
    "                                      (end_sample_clock - start_clock).total_seconds(),\n",
    "                                      (end_loop_clock - start_clock).total_seconds()\n",
    "                                     ]])\n",
    "    \n",
    "result.to_pickle(result_pickle_location + result_pickle_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  Evaluation of Data Release Differential Privacy Methods\n",
    "\n",
    "## Gaussian and Matrixvariate Gaussian Mechanisms by regression task\n",
    "\n",
    "### Identity Query "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_obs, n_features = data.shape\n",
    "\n",
    "evaluation_test_split = 0.1\n",
    "evalaution_test_size = int(n_obs*evaluation_test_split)\n",
    "evaluation_train_size = n_obs - evalaution_test_size\n",
    "\n",
    "evaluation_samples = 20\n",
    "\n",
    "# Model parameters\n",
    "model_params = dict(epochs=5, batch_size=128, verbose=0) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Baseline\n",
    "\n",
    "Evaluation Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# labelling result values for mechanism\n",
    "mechanism = 'baseline'\n",
    "query_type = 'identity'\n",
    "metric = 'rmse'\n",
    "\n",
    "result_pickle_name = '_'.join([mechanism, \n",
    "                               query_type,\n",
    "                               str(sample_size),\n",
    "                               ''.join(response), \n",
    "                               str(evaluation_samples),\n",
    "                               datetime.date.today().strftime(\"%Y%m%d\")])\n",
    "\n",
    "result_pickle_name"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evaluation of baseline model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = None\n",
    "\n",
    "for i in range(evaluation_samples):\n",
    "    start_clock = datetime.datetime.now()\n",
    "    # Train model and evaluate prediction metric on holdout set   \n",
    "    metric_result = seq_nn_single_evaluation(train_data=data,\n",
    "                                             test_data=data,\n",
    "                                             test_holdout_p=evaluation_test_split,\n",
    "                                             X_labels=predictors,\n",
    "                                             y_label=response,\n",
    "                                             fit_params=model_params)\n",
    "    \n",
    "    end_loop_clock = datetime.datetime.now() \n",
    "    \n",
    "    result = record_result(results=result, \n",
    "                           column_names=results_columns, \n",
    "                           new_data=[[mechanism, \n",
    "                                      query_type, \n",
    "                                      sample_size,\n",
    "                                      i+1,\n",
    "                                      metric, \n",
    "                                      metric_result, \n",
    "                                      0,\n",
    "                                      (end_loop_clock - start_clock).total_seconds()\n",
    "                                     ]])\n",
    "    \n",
    "result.to_pickle(result_pickle_location + result_pickle_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Identity Gaussian Mechanism\n",
    "\n",
    "Evaluation Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# labelling result values for mechanism\n",
    "mechanism = 'gaussian'\n",
    "query_type = 'identity'\n",
    "metric = 'rmse'\n",
    "\n",
    "result_pickle_name = '_'.join([mechanism, \n",
    "                               query_type, \n",
    "                               str(sample_size),\n",
    "                               ''.join(response),\n",
    "                               str(evaluation_samples), \n",
    "                               datetime.date.today().strftime(\"%Y%m%d\")])\n",
    "\n",
    "print(result_pickle_name)\n",
    "\n",
    "model_params = dict(epochs=4, batch_size=128, verbose=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Differential Privacy Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evaluation of singleton gaussian sequential NN model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epsilon = 1.0 \n",
    "# 1 / number of observations\n",
    "delta = pow(evaluation_train_size, -1)\n",
    "\n",
    "sensitivity = get_query_point_sensitivity(query_type='identity',\n",
    "                                          query_scale=target_feature_bounds,\n",
    "                                          query_shape=(evaluation_train_size, n_features))\n",
    "\n",
    "result = None\n",
    "\n",
    "for i in range(evaluation_samples):\n",
    "    \n",
    "    start_clock = datetime.datetime.now()\n",
    "    \n",
    "    train_ind, test_ind = \\\n",
    "            test_train_split(len(data),\n",
    "                             evaluation_test_split)\n",
    "        \n",
    "    sample = gaussian_mechanism_matrix_sample(\n",
    "        data=data.iloc[train_ind],\n",
    "        epsilon=epsilon,\n",
    "        delta=delta,\n",
    "        sensitivity=sensitivity,\n",
    "        symmetric=False,\n",
    "        verbose=False)\n",
    "    \n",
    "    end_sample_clock = datetime.datetime.now() \n",
    "        \n",
    "    # Train model and evaluate prediction metric on holdout set   \n",
    "    metric_result = seq_nn_single_evaluation(train_data=sample,\n",
    "                                             test_data=data,\n",
    "                                             X_labels=predictors,\n",
    "                                             y_label=response,\n",
    "                                             train_ind=train_ind, \n",
    "                                             test_ind=test_ind,\n",
    "                                             fit_params=model_params)\n",
    "    \n",
    "    end_loop_clock = datetime.datetime.now() \n",
    "    \n",
    "    result = record_result(results=result, \n",
    "                           column_names=results_columns, \n",
    "                           new_data=[[mechanism, \n",
    "                                      query_type, \n",
    "                                      sample_size,\n",
    "                                      i+1,\n",
    "                                      metric, \n",
    "                                      metric_result, \n",
    "                                      (end_sample_clock - start_clock).total_seconds(),\n",
    "                                      (end_loop_clock - start_clock).total_seconds()\n",
    "                                     ]])\n",
    "    print(result.iloc[i], start_clock, end_loop_clock)\n",
    "    \n",
    "result.to_pickle(result_pickle_location + result_pickle_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Matrix-variate Gaussian Mechanism\n",
    "\n",
    "Binary Allocation Strategy - Key features\n",
    "   \n",
    "    key features = ['age','total_cholesterol','framingham'] \n",
    "    \n",
    "    'age' and 'cholesterol' important as contribute the largest scores to the total. \n",
    "    'framingham' important as the target variable.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evaluation Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# labelling result values for mechanism\n",
    "mechanism = 'MVG_binary_knowledge'\n",
    "query_type = 'identity'\n",
    "metric = 'rmse'\n",
    "\n",
    "result_pickle_name = '_'.join([mechanism, \n",
    "                               query_type, \n",
    "                               str(sample_size),\n",
    "                               ''.join(response),\n",
    "                               str(evaluation_samples), \n",
    "                               datetime.date.today().strftime(\"%Y%m%d\")])\n",
    "\n",
    "print(result_pickle_name)\n",
    "\n",
    "model_params = dict(epochs=5, batch_size=64, verbose=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Differential Privacy Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sensitivity = get_query_row_sensitivity(query_type='identity',\n",
    "                                        query_scale=target_feature_bounds,\n",
    "                                        query_shape=(evaluation_train_size, n_features))\n",
    "\n",
    "gamma = get_query_gamma(query_scale=target_feature_bounds, \n",
    "                        query_shape=(evaluation_train_size, n_features), \n",
    "                        query_type='identity')\n",
    "\n",
    "# Allocation percentages in 'key_features_allocation' to key features \n",
    "# and remainder to all other features\n",
    "key_features_binary_mvg = ['age','total_cholesterol','framingham']  \n",
    "key_features_allocation = [0.45,0.55,0.65,0.75,0.85,0.95]\n",
    "\n",
    "feature_allocations = dict()\n",
    "for allocation in key_features_allocation:\n",
    "    \n",
    "    feature_allocations[allocation] = [ \n",
    "        allocation / len(key_features_binary_mvg)\n",
    "        if feature in key_features_binary_mvg \n",
    "        else (1 - allocation) / (n_features - len(key_features_binary_mvg))\n",
    "        for feature in evaluation_features \n",
    "    ]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Matrix-variate Gaussian Mechnaism Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = None \n",
    "for key, allocation in feature_allocations.items():\n",
    "    \n",
    "    params = dict(\n",
    "        epsilon=epsilon,\n",
    "        delta=delta,\n",
    "        sensitivity=sensitivity,\n",
    "        gamma=gamma,\n",
    "        precision_allocation=allocation,\n",
    "        precision_direction=numpy.identity(n_features),\n",
    "        covariance_direction='unimodal features',\n",
    "        covariance_method='binary'\n",
    "    )\n",
    "    \n",
    "    \n",
    "    for i in range(evaluation_samples):\n",
    "        start_clock = datetime.datetime.now() \n",
    "        train_ind, test_ind = \\\n",
    "            test_train_split(len(data),\n",
    "                             evaluation_test_split)\n",
    "            \n",
    "        sample = matrixvariate_gaussian_mechanism_sample(data=data.iloc[train_ind],\n",
    "                                                         **params)\n",
    "        \n",
    "        end_sample_clock = datetime.datetime.now() \n",
    "        \n",
    "        metric_result = seq_nn_single_evaluation(train_data=sample,\n",
    "                                                test_data=data,\n",
    "                                                X_labels=predictors,\n",
    "                                                y_label=response,\n",
    "                                                train_ind=train_ind, \n",
    "                                                test_ind=test_ind,\n",
    "                                                fit_params=model_params)\n",
    "        \n",
    "        end_loop_clock = datetime.datetime.now() \n",
    "        \n",
    "        result = record_result(results=result, \n",
    "                               column_names=results_columns, \n",
    "                               new_data=[[mechanism + '_' + str(key), \n",
    "                                          query_type, \n",
    "                                          sample_size,\n",
    "                                          i+1,\n",
    "                                          metric, \n",
    "                                          metric_result, \n",
    "                                          (end_sample_clock - start_clock).total_seconds(),\n",
    "                                          (end_loop_clock - start_clock).total_seconds()\n",
    "                                         ]])\n",
    "        \n",
    "result.to_pickle(result_pickle_location + result_pickle_name)         "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Binary Allocation Strategy - Key features\n",
    "   \n",
    "    Features allocations are proprotional to the singular values or explained directional variance\n",
    "    \n",
    "    Directions are equal to eigenvectors of the sample covariance. \n",
    "    These are the orthogonal primary axis of the variation in the sample covariance "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evaluation setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# labelling result values for mechanism\n",
    "mechanism = 'MVG_binary_directed'\n",
    "query_type = 'identity'\n",
    "metric = 'rmse'\n",
    "\n",
    "result_pickle_name = '_'.join([mechanism, \n",
    "                               query_type, \n",
    "                               str(sample_size),\n",
    "                               ''.join(response),\n",
    "                               str(evaluation_samples), \n",
    "                               datetime.date.today().strftime(\"%Y%m%d\")])\n",
    "\n",
    "print(result_pickle_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Differential Privacy parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DP SVD parameters\n",
    "svd_privacy_allocation = 0.2\n",
    "\n",
    "svd_sensitivity = get_query_row_sensitivity(query_type='covariance',\n",
    "                                            query_scale=target_feature_bounds,\n",
    "                                            query_shape=data.shape)\n",
    "\n",
    "# DP MVG parameters\n",
    "\n",
    "sensitivity = get_query_row_sensitivity(query_type='identity',\n",
    "                                        query_scale=target_feature_bounds,\n",
    "                                        query_shape=(evaluation_train_size, n_features))\n",
    "\n",
    "gamma = get_query_gamma(query_scale=target_feature_bounds, \n",
    "                        query_shape=(evaluation_train_size, n_features), \n",
    "                        query_type='identity')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = centered_sample_covariance_matrix(X=data)\n",
    "\n",
    "result = None\n",
    "\n",
    "for i in range(evaluation_samples): \n",
    "\n",
    "    start_svd_clock = datetime.datetime.now()\n",
    "    \n",
    "    cov_sample = gaussian_mechanism_matrix_sample(\n",
    "                data=query,\n",
    "                epsilon=epsilon*svd_privacy_allocation,\n",
    "                delta=delta*svd_privacy_allocation,\n",
    "                sensitivity=svd_sensitivity,\n",
    "                symmetric=True,\n",
    "                verbose=False)\n",
    "\n",
    "    precision_directions, singular_values, _ = numpy.linalg.svd(cov_sample, full_matrices=True)\n",
    "\n",
    "    sv_proportions = singular_values / numpy.sum(singular_values)\n",
    "    sv_allocations = [0.55,0.75,0.95]\n",
    "\n",
    "    feature_allocations = dict()\n",
    "    for allocation in sv_allocations:\n",
    "        feature_allocations[allocation] = [ \n",
    "            ((1 - allocation) / len(sv_proportions)) + \n",
    "            (sv * allocation)\n",
    "            for sv in sv_proportions\n",
    "        ]    \n",
    "\n",
    "    end_svd_clock = datetime.datetime.now()\n",
    "    \n",
    "    for key, allocation in feature_allocations.items():\n",
    "\n",
    "        start_mvg_clock = datetime.datetime.now()\n",
    "        params = dict(\n",
    "            epsilon=epsilon*(1.0-svd_privacy_allocation),\n",
    "            delta=delta*(1.0-svd_privacy_allocation),\n",
    "            sensitivity=sensitivity,\n",
    "            gamma=gamma,\n",
    "            precision_allocation=allocation,\n",
    "            precision_direction=precision_directions,\n",
    "            covariance_direction='unimodal features',\n",
    "            covariance_method='binary'\n",
    "        )        \n",
    "         \n",
    "        train_ind, test_ind = \\\n",
    "            test_train_split(len(data),\n",
    "                             evaluation_test_split)\n",
    "\n",
    "        sample = matrixvariate_gaussian_mechanism_sample(data=data.iloc[train_ind],\n",
    "                                                         **params)\n",
    "\n",
    "        end_mvg_clock = datetime.datetime.now() \n",
    "\n",
    "        metric_result = seq_nn_single_evaluation(train_data=sample,\n",
    "                                                 test_data=data,\n",
    "                                                 X_labels=predictors,\n",
    "                                                 y_label=response,\n",
    "                                                 train_ind=train_ind, \n",
    "                                                 test_ind=test_ind,\n",
    "                                                 fit_params=model_params)\n",
    "\n",
    "        end_loop_clock = datetime.datetime.now() \n",
    "        \n",
    "        sample_clock_dif = (end_svd_clock - start_svd_clock) + (end_mvg_clock - start_mvg_clock)\n",
    "        total_clock_dif =  (end_loop_clock - end_mvg_clock) + sample_clock_dif\n",
    "        \n",
    "        result = record_result(results=result, \n",
    "                               column_names=results_columns, \n",
    "                               new_data=[[mechanism + '_' + str(key), \n",
    "                                          query_type, \n",
    "                                          sample_size,\n",
    "                                          i+1,\n",
    "                                          metric, \n",
    "                                          metric_result, \n",
    "                                          sample_clock_dif.total_seconds(),\n",
    "                                          total_clock_dif.total_seconds()\n",
    "                                         ]])\n",
    "\n",
    "result.to_pickle(result_pickle_location + result_pickle_name) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy\n",
    "import operator\n",
    "import random\n",
    "\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.kernel_ridge import KernelRidge\n",
    "from scipy.stats import uniform\n",
    "\n",
    "import sklearn.metrics\n",
    "import sklearn.preprocessing\n",
    "\n",
    "def root_mean_squared_error(y_true, y_pred):\n",
    "    '''\n",
    "    Computation of evaluation metric RMSE\n",
    "    '''\n",
    "    return pow(sklearn.metrics.mean_squared_error(y_true, y_pred)/ len(y_true), 0.5)\n",
    "\n",
    "\n",
    "def rmse_scorer():\n",
    "    '''\n",
    "    Wrapper function to create sklearn 'scorer' for RMSE metric\n",
    "    '''\n",
    "    return sklearn.metrics.make_scorer(root_mean_squared_error, \n",
    "                                       greater_is_better=False)\n",
    "\n",
    "def kernel_ridge_param_search(\n",
    "        X, y,\n",
    "        scoring,\n",
    "        kernel,\n",
    "        cv_folds=5,\n",
    "        random=True,\n",
    "        max_alpha=3,\n",
    "        max_gamma=3,\n",
    "        random_iters=20):\n",
    "\n",
    "    krr = KernelRidge(kernel=kernel)\n",
    "\n",
    "    if random is True:\n",
    "        # Randomised parameter search\n",
    "        param_dist = dict(alpha=uniform(0.0001, max_alpha),\n",
    "                          gamma=uniform(0.0001, max_gamma))\n",
    "        n_iter_randomised = random_iters\n",
    "\n",
    "        param_search = RandomizedSearchCV(\n",
    "            estimator=krr,\n",
    "            param_distributions=param_dist,\n",
    "            n_iter=n_iter_randomised,\n",
    "            scoring=scoring,\n",
    "            n_jobs=-1,\n",
    "            pre_dispatch='2*n_jobs',\n",
    "            cv=cv_folds)\n",
    "    else:\n",
    "        # Grid parameter search\n",
    "        param_grid = dict(alpha=numpy.arange(0.1, max_alpha, 0.1),\n",
    "                          gamma=numpy.arange(0.1, max_gamma, 0.1))\n",
    "\n",
    "        param_search = GridSearchCV(\n",
    "            estimator=krr,\n",
    "            param_grid=param_grid,\n",
    "            scoring=scoring,\n",
    "            cv=cv_folds)\n",
    "\n",
    "    return param_search.fit(X, y).best_params_\n",
    "\n",
    "\n",
    "def kernel_ridge_cv(X, y, scoring, cv_folds, **kwargs):\n",
    "\n",
    "    # Apply Kernel Ridge Regression with empirically selected parameters\n",
    "    KRR = KernelRidge(**kwargs)\n",
    "\n",
    "    # Calculate metric RMSE using\n",
    "    cv_score = cross_val_score(\n",
    "        estimator=KRR,\n",
    "        X=X,\n",
    "        y=y,\n",
    "        scoring=scoring,\n",
    "        cv=cv_folds)\n",
    "\n",
    "    return numpy.array(abs(cv_score))\n",
    "\n",
    "\n",
    "def krr_private_cross_validate(train_X,\n",
    "                               train_y,\n",
    "                               test_X,\n",
    "                               test_y,\n",
    "                               krr,\n",
    "                               scoring_function,\n",
    "                               score_summary_function=None,\n",
    "                               cv_folds=5):\n",
    "\n",
    "    train_selected = [ i for i in range(len(train_y))]\n",
    "    fold_sample_size = int(len(train_y) / cv_folds)\n",
    "\n",
    "    res = numpy.empty(cv_folds)\n",
    "\n",
    "    for fold in range(cv_folds):\n",
    "\n",
    "        fold_sample = [\n",
    "            train_selected.pop(random.randrange(len(train_selected)))\n",
    "            for _ in range(fold_sample_size)]\n",
    "\n",
    "        krr.fit(train_X[fold_sample],\n",
    "                train_y[fold_sample])\n",
    "\n",
    "        y_hat = krr.predict(test_X[fold_sample])\n",
    "        \n",
    "        res[fold] = scoring_function(y_true=test_y[fold_sample], y_pred=y_hat)\n",
    "\n",
    "    if score_summary_function is not None:\n",
    "        summary_res = score_summary_function(res)\n",
    "    else:\n",
    "        summary_res = res\n",
    "\n",
    "    return summary_res\n",
    "\n",
    "\n",
    "def krr_private_param_rand_search(train_X,\n",
    "                                  train_y,\n",
    "                                  test_X,\n",
    "                                  test_y,\n",
    "                                  rand_iters,\n",
    "                                  krr_kernel,\n",
    "                                  scoring_function,\n",
    "                                  train_split=0.8):\n",
    "\n",
    "    # Randomised parameter search\n",
    "    param_selections = dict()\n",
    "    param_dist = dict(alpha=uniform(0.0001, 4),\n",
    "                      gamma=uniform(0.0001, 4))\n",
    "\n",
    "    for _ in range(rand_iters):\n",
    "\n",
    "        a = round(param_dist['alpha'].rvs(1)[0], 4)\n",
    "        g = round(param_dist['gamma'].rvs(1)[0], 4)\n",
    "\n",
    "        model = KernelRidge(kernel=krr_kernel, alpha=a, gamma=g)\n",
    "\n",
    "        param_selections[(a, g)] = krr_private_cross_validate(\n",
    "            train_X=train_X,\n",
    "            train_y=train_y,\n",
    "            test_X=test_X,\n",
    "            test_y=test_y,\n",
    "            krr=model,\n",
    "            score_summary_function=numpy.mean,\n",
    "            scoring_function=scoring_function)\n",
    "\n",
    "    best_alpha, best_gamma = \\\n",
    "        min(param_selections.items(), key=operator.itemgetter(1))[0]\n",
    "\n",
    "    return dict(\n",
    "        best_alpha=best_alpha,\n",
    "        best_gamma=best_gamma,\n",
    "        best_rmse=min(param_selections.values()),\n",
    "        results=param_selections)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_train_split(y_len, test_perc):\n",
    "    if test_perc >= 0.0 and test_perc <= 1.0:\n",
    "        selection_pool = [ i for i in range(y_len)]\n",
    "        test_size = int(y_len * (1.0 - test_perc))\n",
    "\n",
    "        selected = [\n",
    "            selection_pool.pop(random.randrange(len(selection_pool)))\n",
    "            for _ in range(test_size)\n",
    "        ]\n",
    "\n",
    "        return selected, selection_pool\n",
    "    else:\n",
    "        print('test_train_split_indicies: \\tparameter \"test_perc\" must have ')\n",
    "        print('\\t\\t\\t\\tvalue between 0 and 1, not {p}'.format(p=test_perc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
