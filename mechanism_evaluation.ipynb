{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "import pandas\n",
    "import numpy\n",
    "import glob\n",
    "import re\n",
    "\n",
    "# local imports\n",
    "from differential_privacy_parameters import get_query_point_sensitivity\n",
    "from differential_privacy_parameters import get_query_row_sensitivity\n",
    "from differential_privacy_parameters import get_query_gamma\n",
    "\n",
    "from differential_privacy_mechanisms import gaussian_mechanism_matrix_sample\n",
    "from differential_privacy_mechanisms import MVGMechanism\n",
    "\n",
    "from model_evaluation import test_train_split\n",
    "from model_evaluation import principle_component_RSS\n",
    "from model_evaluation import root_mean_squared_error\n",
    "from model_evaluation import record_result\n",
    "\n",
    "from preprocessing import centered_sample_covariance_matrix\n",
    "from preprocessing import scale_data\n",
    "\n",
    "from models import seq_nn_single_evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  Data Processing and Setup\n",
    "\n",
    "Import and concatonate all data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading...\tdata/Florida_100000_20190227\n",
      "Loading...\tdata/Ohio_100000_20190227\n",
      "Loading...\tdata/Pennsylvania_100000_20190227\n",
      "Loading...\tdata/Illinois_100000_20190227\n",
      "Loading...\tdata/Texas_100000_20190227\n",
      "Loading...\tdata/California_100000_20190227\n",
      "Loading...\tdata/Georgia_100000_20190227\n",
      "Loading...\tdata/New York_100000_20190227\n"
     ]
    }
   ],
   "source": [
    "target_dir = 'data/'\n",
    "\n",
    "data_load = None\n",
    "for file_name in glob.glob(target_dir + '*'):\n",
    "    if not(re.search(r'\\.data$',file_name)):\n",
    "        print('Loading...\\t' + file_name)\n",
    "        if data_load is None:\n",
    "            data_load = pandas.read_pickle(file_name)\n",
    "        else:\n",
    "            data_load = pandas.concat([data_load,\n",
    "                                       pandas.read_pickle(file_name)], \n",
    "                                      sort=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_load.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Scale data and establish evaluation parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluation_samples = 100\n",
    "\n",
    "evaluation_features = [\n",
    "    'bmi',\n",
    "    'diastolic_blood_pressure',\n",
    "    'systolic_blood_pressure',\n",
    "    'glucose',\n",
    "    'hdl_cholesterol',\n",
    "    'ldl_cholesterol',\n",
    "    'total_cholesterol',\n",
    "    'triglycerides',\n",
    "    'age',\n",
    "    'framingham'    \n",
    "]\n",
    "\n",
    "data_feature_bounds = {\n",
    "    'bmi':(0,400),\n",
    "    'diastolic_blood_pressure':(60,140),\n",
    "    'systolic_blood_pressure':(90,250),\n",
    "    'glucose':(0,2000),\n",
    "    'hdl_cholesterol':(0,1500),\n",
    "    'ldl_cholesterol':(0,2000),\n",
    "    'total_cholesterol':(0,2100),\n",
    "    'triglycerides':(0,3000),\n",
    "    'age':(0,120),\n",
    "    'framingham':(-10,37)\n",
    "}\n",
    "target_feature_bounds = (0,1)\n",
    "\n",
    "# Setup for estimation of framingham score\n",
    "response = ['framingham']\n",
    "predictors = [ f for f in evaluation_features if f not in response]\n",
    "\n",
    "results_columns = [\n",
    "    'mechanism', \n",
    "    'query', \n",
    "    'iteration', \n",
    "    'metric', \n",
    "    'result', \n",
    "    'mechanism runtime (s)', \n",
    "    'total runtime (s)'\n",
    "]\n",
    "\n",
    "# Scale data 'data_feature_bounds' -> 'target_feature_bounds'\n",
    "data = scale_data(data_load[evaluation_features].dropna(),\n",
    "                  target_bounds=target_feature_bounds,\n",
    "                  data_bounds=data_feature_bounds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Differential Privacy parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "epsilon = 1.0 \n",
    "# 1 / number of observations\n",
    "delta = pow(data.shape[0], -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation of Sample Covariance Differential Privacy Methods\n",
    "\n",
    "## Gaussian Mechanism with symmetric and identity sampling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Symmetric Gaussian Mechanism\n",
    "\n",
    "Evaluation setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# labelling result values for mechanism\n",
    "mechanism = 'gaussian'\n",
    "query_type = 'covariance'\n",
    "metric = 'principle component RSS'\n",
    "\n",
    "result_pickle_location = 'results/'\n",
    "result_pickle_name = '_'.join([mechanism, \n",
    "                               query_type, \n",
    "                               str(evaluation_samples), \n",
    "                               datetime.date.today().strftime(\"%Y%m%d\")])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Differential Privacy parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epsilon = 1.0 \n",
    "# 1 / number of observations\n",
    "delta = pow(data.shape[0], -1)\n",
    "\n",
    "sensitivity = get_query_row_sensitivity(query_type='covariance',\n",
    "                                        query_scale=target_feature_bounds,\n",
    "                                        query_shape=data.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evaluation of symmetric matrix gaussian mechanism sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = centered_sample_covariance_matrix(X=data)\n",
    "\n",
    "result = None\n",
    "sample = dict()\n",
    "\n",
    "for i in xrange(evaluation_samples): \n",
    "    # Sample mechanism\n",
    "    start_clock = datetime.datetime.now()\n",
    "    # Add symmetric iid noise\n",
    "    sample[i] = gaussian_mechanism_matrix_sample(\n",
    "                data=query,\n",
    "                epsilon=epsilon,\n",
    "                delta=delta,\n",
    "                sensitivity=sensitivity,\n",
    "                symmetric=True,\n",
    "                verbose=False)\n",
    "    end_sample_clock = datetime.datetime.now() \n",
    "\n",
    "    result = record_result(results=result, \n",
    "                           column_names=results_columns, \n",
    "                           new_data=[[mechanism, \n",
    "                                      query_type, \n",
    "                                      i+1,\n",
    "                                      metric, \n",
    "                                      principle_component_RSS(true=query, pred=sample[i]), \n",
    "                                      (end_sample_clock - start_clock).total_seconds(),\n",
    "                                      (end_sample_clock - start_clock).total_seconds()\n",
    "                                     ]])\n",
    "    \n",
    "result.to_pickle(result_pickle_location + result_pickle_name)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Identity Gaussian Mechanism\n",
    "\n",
    "Evaluation setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# labelling result values for mechanism\n",
    "mechanism = 'gaussian'\n",
    "query_type = 'itentity'\n",
    "metric = 'principle component RSS'\n",
    "\n",
    "result_pickle_name = '_'.join([mechanism, \n",
    "                               query_type, \n",
    "                               str(evaluation_samples), \n",
    "                               datetime.date.today().strftime(\"%Y%m%d\")])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Differential Privacy parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sensitivity = get_query_row_sensitivity(query_type='singleton',\n",
    "                                        query_scale=target_feature_bounds,\n",
    "                                        query_shape=data.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evaluation of identity query guassian mechanism sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = centered_sample_covariance_matrix(X=data)\n",
    "\n",
    "result = None\n",
    "\n",
    "for i in xrange(evaluation_samples): \n",
    "    # Sample mechanism\n",
    "    start_clock = datetime.datetime.now()\n",
    "    # Add symmetric iid noise\n",
    "    sample = gaussian_mechanism_matrix_sample(\n",
    "                data=query,\n",
    "                epsilon=epsilon,\n",
    "                delta=delta,\n",
    "                sensitivity=sensitivity,\n",
    "                symmetric=False,\n",
    "                verbose=False)\n",
    "    end_sample_clock = datetime.datetime.now() \n",
    "\n",
    "    sample_cov = centered_sample_covariance_matrix(X=sample)\n",
    "    end_loop_clock = datetime.datetime.now() \n",
    "    \n",
    "    result = record_result(results=result, \n",
    "                           column_names=results_columns, \n",
    "                           new_data=[[mechanism, \n",
    "                                      query_type, \n",
    "                                      i+1,\n",
    "                                      metric, \n",
    "                                      principle_component_RSS(true=query, pred=sample_cov), \n",
    "                                      (end_sample_clock - start_clock).total_seconds(),\n",
    "                                      (end_loop_clock - start_clock).total_seconds()\n",
    "                                     ]])\n",
    "    \n",
    "result.to_pickle(result_pickle_location + result_pickle_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  Evaluation of Data Release Differential Privacy Methods\n",
    "\n",
    "## Gaussian and Matrixvariate Gaussian Mechanisms by regression task\n",
    "\n",
    "### Identity Query "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_obs, n_features = data.shape\n",
    "\n",
    "evaluation_test_split = 0.1\n",
    "evalaution_test_size = int(n_obs*evaluation_test_split)\n",
    "evaluation_train_size = n_obs - evalaution_test_size\n",
    "\n",
    "evaluation_samples = 2\n",
    "\n",
    "# Model parameters\n",
    "model_params = dict(epochs=5, batch_size=32, verbose=0) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Baseline\n",
    "\n",
    "Evaluation Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# labelling result values for mechanism\n",
    "mechanism = 'baseline'\n",
    "query_type = 'identity'\n",
    "metric = 'rmse'\n",
    "\n",
    "result_pickle_name = '_'.join([mechanism, \n",
    "                               query_type,\n",
    "                               ''.join(response), \n",
    "                               str(evaluation_samples), \n",
    "                               datetime.date.today().strftime(\"%Y%m%d\")])\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evaluation of baseline model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = None\n",
    "\n",
    "for i in xrange(evaluation_samples):\n",
    "    start_clock = datetime.datetime.now()\n",
    "    # Train model and evaluate prediction metric on holdout set   \n",
    "    metric_result = seq_nn_single_evaluation(train_data=data,\n",
    "                                             test_data=data,\n",
    "                                             test_holdout_p=evaluation_test_split,\n",
    "                                             X_labels=predictors,\n",
    "                                             y_label=response,\n",
    "                                             fit_params=model_params)\n",
    "    \n",
    "    end_loop_clock = datetime.datetime.now() \n",
    "    \n",
    "    result = record_result(results=result, \n",
    "                           column_names=results_columns, \n",
    "                           new_data=[[mechanism, \n",
    "                                      query_type, \n",
    "                                      i+1,\n",
    "                                      metric, \n",
    "                                      metric_result, \n",
    "                                      0,\n",
    "                                      (end_loop_clock - start_clock).total_seconds()\n",
    "                                     ]])\n",
    "    \n",
    "result.to_pickle(result_pickle_location + result_pickle_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result.decsribe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Singleton Gaussian Mechanism\n",
    "\n",
    "Evaluation Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# labelling result values for mechanism\n",
    "mechanism = 'gaussian'\n",
    "query_type = 'identity'\n",
    "metric = 'rmse'\n",
    "\n",
    "result_pickle_name = '_'.join([mechanism, \n",
    "                               query_type, \n",
    "                               ''.join(response),\n",
    "                               str(evaluation_samples), \n",
    "                               datetime.date.today().strftime(\"%Y%m%d\")])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Differential Privacy Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epsilon = 1.0 \n",
    "# 1 / number of observations\n",
    "delta = pow(evaluation_train_size, -1)\n",
    "\n",
    "sensitivity = get_query_point_sensitivity(query_type='singleton',\n",
    "                                          query_scale=target_feature_bounds,\n",
    "                                          query_shape=(evaluation_train_size, n_features))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evaluation of singleton gaussian sequential NN model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = None\n",
    "\n",
    "for i in xrange(evaluation_samples):\n",
    "    \n",
    "    start_clock = datetime.datetime.now()\n",
    "    \n",
    "    train_ind, test_ind = \\\n",
    "            test_train_split(len(data),\n",
    "                             evaluation_test_split)\n",
    "        \n",
    "    sample = gaussian_mechanism_matrix_sample(\n",
    "        data=data.iloc[train_ind],\n",
    "        epsilon=epsilon,\n",
    "        delta=delta,\n",
    "        sensitivity=sensitivity,\n",
    "        symmetric=False,\n",
    "        verbose=False)\n",
    "    \n",
    "    end_sample_clock = datetime.datetime.now() \n",
    "        \n",
    "    # Train model and evaluate prediction metric on holdout set   \n",
    "    metric_result = seq_nn_single_evaluation(train_data=sample,\n",
    "                                             test_data=data,\n",
    "                                             X_labels=predictors,\n",
    "                                             y_label=response,\n",
    "                                             train_ind=train_ind, \n",
    "                                             test_ind=test_ind,\n",
    "                                             fit_params=model_params)\n",
    "    \n",
    "    end_loop_clock = datetime.datetime.now() \n",
    "    \n",
    "    result = record_result(results=result, \n",
    "                           column_names=results_columns, \n",
    "                           new_data=[[mechanism, \n",
    "                                      query_type, \n",
    "                                      i+1,\n",
    "                                      metric, \n",
    "                                      metric_result, \n",
    "                                      (end_sample_clock - start_clock).total_seconds(),\n",
    "                                      (end_loop_clock - start_clock).total_seconds()\n",
    "                                     ]])\n",
    "    \n",
    "result.to_pickle(result_pickle_location + result_pickle_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Matrix-variate Gaussian Mechanism\n",
    "\n",
    "Binary Allocation Strategy - Key features\n",
    "   \n",
    "    key features = ['age','total_cholesterol','framingham'] \n",
    "    \n",
    "    'age' and 'cholesterol' important as contribute the largest scores to the total. \n",
    "    'framingham' important as the target variable.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evaluation Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MVG_binary_knowledge_identity_framingham_2_20190227\n"
     ]
    }
   ],
   "source": [
    "# labelling result values for mechanism\n",
    "query_type = 'identity'\n",
    "metric = 'rmse'\n",
    "\n",
    "result_pickle_name = '_'.join([mechanism, \n",
    "                               query_type, \n",
    "                               ''.join(response),\n",
    "                               str(evaluation_samples), \n",
    "                               datetime.date.today().strftime(\"%Y%m%d\")])\n",
    "\n",
    "print(result_pickle_name)\n",
    "\n",
    "model_params = dict(epochs=10, batch_size=16, verbose=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Differential Privacy Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sensitivity = get_query_row_sensitivity(query_type='identity',\n",
    "                                        query_scale=target_feature_bounds,\n",
    "                                        query_shape=(evaluation_train_size, n_features))\n",
    "\n",
    "gamma = get_query_gamma(query_scale=target_feature_bounds, \n",
    "                        query_shape=(evaluation_train_size, n_features), \n",
    "                        query_type='identity')\n",
    "\n",
    "# Allocation percentages in 'key_features_allocation' to key features \n",
    "# and remainder to all other features\n",
    "key_features_binary_mvg = ['age','total_cholesterol','framingham']  \n",
    "key_features_allocation = [0.45,0.55,0.65,0.75,0.85,0.95]\n",
    "\n",
    "feature_allocations = dict()\n",
    "for allocation in key_features_allocation:\n",
    "    \n",
    "    feature_allocations[allocation] = [ \n",
    "        allocation / len(key_features_binary_mvg)\n",
    "        if feature in key_features_binary_mvg \n",
    "        else (1 - allocation) / (n_features - len(key_features_binary_mvg))\n",
    "        for feature in evaluation_features \n",
    "    ]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Matrix-variate Gaussian Mechnaism Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = None \n",
    "for key, allocation in feature_allocations.items():\n",
    "    \n",
    "    params = dict(\n",
    "        epsilon=epsilon,\n",
    "        delta=delta,\n",
    "        sensitivity=sensitivity,\n",
    "        gamma=gamma,\n",
    "        precision_allocation=allocation,\n",
    "        precision_direction=numpy.identity(features),\n",
    "        covariance_direction='unimodal features',\n",
    "        covariance_method='binary'\n",
    "    )\n",
    "    \n",
    "    mechanism = 'MVG_binary_knowledge_' + str(key)\n",
    "    \n",
    "    for i in xrange(evaluation_samples):\n",
    "        start_clock = datetime.datetime.now() \n",
    "        train_ind, test_ind = \\\n",
    "            test_train_split(len(data),\n",
    "                             evaluation_test_split)\n",
    "            \n",
    "        sample = matrixvariate_gaussian_mechanism_sample(data=data.iloc[train_ind],\n",
    "                                                         **params)\n",
    "        \n",
    "        end_sample_clock = datetime.datetime.now() \n",
    "        \n",
    "        metric_result = seq_nn_cross_validation(train_data=sample,\n",
    "                                                test_data=data,\n",
    "                                                X_labels=predictors,\n",
    "                                                y_label=response,\n",
    "                                                train_ind=train_ind, \n",
    "                                                test_ind=test_ind,\n",
    "                                                fit_params=model_params)\n",
    "        \n",
    "        end_loop_clock = datetime.datetime.now() \n",
    "        \n",
    "        result = record_result(results=result, \n",
    "                               column_names=results_columns, \n",
    "                               new_data=[[mechanism, \n",
    "                                          query_type, \n",
    "                                          i+1,\n",
    "                                          metric, \n",
    "                                          metric_result, \n",
    "                                          (end_sample_clock - start_clock).total_seconds(),\n",
    "                                          (end_loop_clock - start_clock).total_seconds()\n",
    "                                         ]])\n",
    "        \n",
    "            "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Binary Allocation Strategy - Key features\n",
    "   \n",
    "    Features allocations are proprotional to the singular values or explained directional variance\n",
    "    \n",
    "    Directions are equal to eigenvectors of the sample covariance. \n",
    "    These are the orthogonal primary axis of the variation in the sample covariance "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evaluation setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MVG_binary_directed_identity_framingham_2_20190227\n"
     ]
    }
   ],
   "source": [
    "# labelling result values for mechanism\n",
    "query_type = 'identity'\n",
    "metric = 'rmse'\n",
    "\n",
    "result_pickle_name = '_'.join([mechanism, \n",
    "                               query_type, \n",
    "                               ''.join(response),\n",
    "                               str(evaluation_samples), \n",
    "                               datetime.date.today().strftime(\"%Y%m%d\")])\n",
    "\n",
    "print(result_pickle_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Differential Privacy parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DP SVD parameters\n",
    "svd_privacy_allocation = 0.2\n",
    "\n",
    "svd_sensitivity = get_query_row_sensitivity(query_type='covariance',\n",
    "                                            query_scale=target_feature_bounds,\n",
    "                                            query_shape=data.shape)\n",
    "\n",
    "# DP MVG parameters\n",
    "\n",
    "sensitivity = get_query_row_sensitivity(query_type='identity',\n",
    "                                        query_scale=target_feature_bounds,\n",
    "                                        query_shape=(evaluation_train_size, n_features))\n",
    "\n",
    "gamma = get_query_gamma(query_scale=target_feature_bounds, \n",
    "                        query_shape=(evaluation_train_size, n_features), \n",
    "                        query_type='identity')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0.55: [0.23660221233706052,\n",
       "  0.130977369594363,\n",
       "  0.11656795262303468,\n",
       "  0.10614268849937145,\n",
       "  0.09606488030774546,\n",
       "  0.0817687274839502,\n",
       "  0.07243609779425597,\n",
       "  0.05761423299606699,\n",
       "  0.05648034212488043,\n",
       "  0.04534549623927129],\n",
       " 0.75: [0.28627574409599166,\n",
       "  0.14224186762867683,\n",
       "  0.12259266266777458,\n",
       "  0.1083763934082338,\n",
       "  0.09463392769238016,\n",
       "  0.07513917384175026,\n",
       "  0.062412860628530874,\n",
       "  0.042201226812818626,\n",
       "  0.040655011988473315,\n",
       "  0.025471131235369943],\n",
       " 0.95: [0.3359492758549227,\n",
       "  0.15350636566299064,\n",
       "  0.12861737271251444,\n",
       "  0.11061009831709614,\n",
       "  0.09320297507701489,\n",
       "  0.06850962019955033,\n",
       "  0.052389623462805776,\n",
       "  0.02678822062957026,\n",
       "  0.0248296818520662,\n",
       "  0.005596766231468598]}"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query = centered_sample_covariance_matrix(X=data)\n",
    "\n",
    "cov_sample = gaussian_mechanism_matrix_sample(\n",
    "                data=query,\n",
    "                epsilon=epsilon*svd_privacy_allocation,\n",
    "                delta=delta*svd_privacy_allocation,\n",
    "                sensitivity=svd_sensitivity,\n",
    "                symmetric=True,\n",
    "                verbose=False)\n",
    "\n",
    "precision_directions, singular_values, _ = numpy.linalg.svd(cov_sample, full_matrices=True)\n",
    "\n",
    "sv_proportions = singular_values / numpy.sum(singular_values)\n",
    "sv_allocations = [0.55,0.75,0.95]\n",
    "\n",
    "feature_allocations = dict()\n",
    "for allocation in sv_allocations:\n",
    "    feature_allocations[allocation] = [ \n",
    "        ((1 - allocation) / len(sv_proportions)) + \n",
    "        (sv * allocation)\n",
    "        for sv in sv_proportions\n",
    "    ]\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for allocation in feature_allocations    \n",
    "    mechanism = 'MVG_binary_directed_' + str(allocation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from model_evaluation import cross_validation_split\n",
    "from model_evaluation import test_train_split\n",
    "from model_evaluation import root_mean_squared_error\n",
    "from tensorflow.keras import metrics\n",
    "from tensorflow.keras import layers\n",
    "import tensorflow as tf\n",
    "\n",
    "# Remove below if with to use GPU\n",
    "import os\n",
    "os.environ[\"CUDA_DEVICE_ORDER\"] = \"PCI_BUS_ID\"   # see issue #152\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = '-1'\n",
    "\n",
    "\n",
    "def keras_seq_reg_model_compilation(features, tf_loss, tf_metrics):\n",
    "\n",
    "    model_layers = [\n",
    "        layers.Dense(features, activation='relu', kernel_initializer='normal',\n",
    "                     kernel_regularizer=tf.keras.regularizers.l2(0.01)),\n",
    "\n",
    "        layers.Dense(int(features/2), activation='relu',\n",
    "                     kernel_initializer='normal'),\n",
    "\n",
    "        layers.Dense(1, kernel_initializer='normal')\n",
    "    ]\n",
    "\n",
    "    model = tf.keras.Sequential(model_layers)\n",
    "\n",
    "    model.compile(optimizer=tf.train.AdamOptimizer(0.001),\n",
    "                  loss=tf_loss,\n",
    "                  metrics=tf_metrics)\n",
    "\n",
    "    return model\n",
    "\n",
    "\n",
    "def seq_nn_cross_validation(train_data,\n",
    "                            test_data,\n",
    "                            folds,\n",
    "                            X_labels,\n",
    "                            y_label,\n",
    "                            fit_params):\n",
    "\n",
    "    result = list()\n",
    "    for train_ind, test_ind in cross_validation_split(len(train_data),\n",
    "                                                      folds=folds):\n",
    "\n",
    "        train = train_data.iloc[train_ind]\n",
    "        X_train = train[X_labels].values\n",
    "        y_train = train[y_label].values\n",
    "\n",
    "        test = test_data.iloc[test_ind]\n",
    "        X_test = test[X_labels].values\n",
    "        y_test = test[y_label].values\n",
    "\n",
    "        # Create sequential NN model\n",
    "        model = keras_seq_reg_model_compilation(\n",
    "            features=X_test.shape[1],\n",
    "            tf_loss=tf.losses.mean_squared_error,\n",
    "            tf_metrics=[metrics.mean_squared_error,\n",
    "                        metrics.kullback_leibler_divergence,\n",
    "                        metrics.mean_absolute_error])\n",
    "\n",
    "        # Train model\n",
    "        model.fit(X_train,\n",
    "                  y_train,\n",
    "#                   validation_data=(X_test, y_test),\n",
    "                  **fit_params)\n",
    "\n",
    "        if 'batch_size' not in fit_params.keys():\n",
    "            fit_params['batch_size'] = 32\n",
    "\n",
    "        result += [root_mean_squared_error(\n",
    "            model.predict(X_test, batch_size=fit_params['batch_size']),\n",
    "            y_test)]\n",
    "\n",
    "    return result\n",
    "\n",
    "\n",
    "def seq_nn_single_evaluation(train_data,\n",
    "                             test_data,\n",
    "                             X_labels,\n",
    "                             y_label,\n",
    "                             fit_params,\n",
    "                             test_holdout_p=None,\n",
    "                             train_ind=None,\n",
    "                             test_ind=None):\n",
    "\n",
    "    if train_ind is None or test_ind is None:\n",
    "        train_ind, test_ind = \\\n",
    "            test_train_split(len(test_data),\n",
    "                             test_holdout_p)\n",
    "\n",
    "        train = train_data.iloc[train_ind]\n",
    "        X_train = train[X_labels].values\n",
    "        y_train = train[y_label].values\n",
    "    else:\n",
    "        X_train = train_data[X_labels].values\n",
    "        y_train = train_data[y_label].values\n",
    "\n",
    "    test = test_data.iloc[test_ind]\n",
    "    X_test = test[X_labels].values\n",
    "    y_test = test[y_label].values\n",
    "\n",
    "    # Create sequential NN model\n",
    "    model = keras_seq_reg_model_compilation(\n",
    "        features=X_test.shape[1],\n",
    "        tf_loss=tf.losses.mean_squared_error,\n",
    "        tf_metrics=[metrics.mean_squared_error,\n",
    "                    metrics.kullback_leibler_divergence,\n",
    "                    metrics.mean_absolute_error])\n",
    "\n",
    "    # Train model\n",
    "    model.fit(X_train,\n",
    "              y_train,\n",
    "#               validation_data=(X_test, y_test),\n",
    "              **fit_params)\n",
    "\n",
    "    if 'batch_size' not in fit_params.keys():\n",
    "        fit_params['batch_size'] = 32\n",
    "\n",
    "    result = root_mean_squared_error(\n",
    "        model.predict(X_test, batch_size=fit_params['batch_size']),\n",
    "        y_test)\n",
    "\n",
    "    return result\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy\n",
    "\n",
    "# Establish parameters: gaussian_sensitivity, MVG_sensitivity\n",
    "\n",
    "\n",
    "def centered_covariance_query_sensitivity(n, m, c):\n",
    "    '''\n",
    "       'A Differential Privacy Mechanism Design Under Matrix-Valued Query'\n",
    "        Chanyaswad, Dytso, Poor & Mittal 2018, p.18.:\n",
    "        https://arxiv.org/abs/1802.10077 (accessed 16/12/2018) [1]\n",
    "\n",
    "       Requires\n",
    "           n   - divisor of query function\n",
    "           m   - number of unit values / observations to be varied\n",
    "                 under adjacency definition\n",
    "           c   - maximum possible value in range of single observation\n",
    "\n",
    "       Returns - Sensitivity calculation for zero mean\n",
    "                 covariance estimation query\n",
    "                 ie f(X) = n^-1 * transpose(X)X\n",
    "    '''\n",
    "    return (2 * float(m) * float(c)**2) / float(n)\n",
    "\n",
    "\n",
    "'''\n",
    "    Examples:\n",
    "        get_query_point_sensitivity(query_type='identity',\n",
    "                                    query_scale=feature_scale,\n",
    "                                    query_shape=(train_size,features)\n",
    "        get_query_point_sensitivity(query_type='covariance',\n",
    "                                    query_scale=feature_scale,\n",
    "                                    query_shape=(train_size,features)\n",
    "'''\n",
    "\n",
    "\n",
    "def get_query_point_sensitivity(query_scale, query_shape, query_type):\n",
    "    if query_type == 'identity':\n",
    "        # Global maximum of change for any\n",
    "        # single point change in query f(X) = X\n",
    "        result = abs(numpy.subtract(query_scale[0], query_scale[1]))\n",
    "    elif query_type == 'covariance':\n",
    "        # Global maximum of change for a single point\n",
    "        # change in query f(X) = (1/n)*transpose(X)X\n",
    "        result = centered_covariance_query_sensitivity(n=query_shape[0],\n",
    "                                                       m=1.0,\n",
    "                                                       c=query_scale[1])\n",
    "    else:\n",
    "        print(\"get_query_point_sensitivity: \\\n",
    "        required query_type in ('identity','covariance')\")\n",
    "        result = None\n",
    "    return result\n",
    "\n",
    "\n",
    "'''\n",
    "    Examples:\n",
    "        get_query_row_sensitivity(query_type='identity',\n",
    "                                  query_scale=feature_scale,\n",
    "                                  query_shape=(train_size,features))\n",
    "        get_query_row_sensitivity(query_type='covariance',\n",
    "                                  query_scale=feature_scale,\n",
    "                                  query_shape=(train_size,features))\n",
    "'''\n",
    "\n",
    "\n",
    "def get_query_row_sensitivity(query_scale, query_shape, query_type):\n",
    "    if query_type == 'identity':\n",
    "        # Global maximum of change for any data row / observation\n",
    "        # change in query f(X) = X\n",
    "        # Section 8.1.3 p30 of paper [1]\n",
    "        sensitivity = get_query_point_sensitivity(query_scale,\n",
    "                                                  query_shape,\n",
    "                                                  'identity')\n",
    "        result = pow(query_shape[1] * pow(sensitivity, 2),\n",
    "                     0.5)\n",
    "    elif query_type == 'covariance':\n",
    "        # Global maximum of change for any data\n",
    "        # row / observation change in query f(X) = n^-1 * transpose(X)X\n",
    "        result = centered_covariance_query_sensitivity(n=query_shape[0],\n",
    "                                                       m=query_shape[1],\n",
    "                                                       c=query_scale[1])\n",
    "    else:\n",
    "        print(\"get_query_row_sensitivity: \\\n",
    "        required query_type in ('identity','covariance')\")\n",
    "        result = None\n",
    "    return result\n",
    "\n",
    "\n",
    "'''\n",
    "    Examples:\n",
    "        get_query_gamma(feature_scale,\n",
    "                        (train_size,features),\n",
    "                        'identity')\n",
    "        get_query_gamma(feature_scale,\n",
    "                        (features,features),\n",
    "                        query_type='covariance'))\n",
    "'''\n",
    "\n",
    "\n",
    "def get_query_gamma(query_scale, query_shape, query_type):\n",
    "    obs, features = query_shape\n",
    "    if query_type == 'identity':\n",
    "        # Defined as sup X ||f(X)||F, where ||f(X)||F is the\n",
    "        # Frobenious norm of the query f(X)\n",
    "        result = pow(obs * features * query_scale[1], 0.5)\n",
    "    elif query_type == 'covariance':\n",
    "        # Defined as m * c^2 Section 4.4 example 1, p17 in [1]\n",
    "        result = features*pow(query_scale[1], 2)\n",
    "    else:\n",
    "        print(\"get_query_gamma: \\\n",
    "        required query_type in ('identity','covariance')\")\n",
    "        result = None\n",
    "    return result\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
