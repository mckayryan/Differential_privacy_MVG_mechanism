{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "import pandas\n",
    "import numpy\n",
    "import glob\n",
    "import re\n",
    "\n",
    "# local imports\n",
    "from differential_privacy_parameters import get_query_point_sensitivity\n",
    "from differential_privacy_parameters import get_query_row_sensitivity\n",
    "from differential_privacy_parameters import get_query_gamma\n",
    "\n",
    "from differential_privacy_mechanisms import gaussian_mechanism_matrix_sample\n",
    "from differential_privacy_mechanisms import MVGMechanism\n",
    "\n",
    "from model_evaluation import test_train_split\n",
    "from model_evaluation import principle_component_RSS\n",
    "from model_evaluation import root_mean_squared_error\n",
    "from model_evaluation import record_result\n",
    "\n",
    "from preprocessing import centered_sample_covariance_matrix\n",
    "from preprocessing import scale_data\n",
    "\n",
    "from models import seq_nn_single_evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  Data Processing and Setup\n",
    "\n",
    "Import and concatonate all data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading...\tdata/Florida_100000_20190227\n",
      "Loading...\tdata/Ohio_100000_20190227\n",
      "Loading...\tdata/Pennsylvania_100000_20190227\n",
      "Loading...\tdata/Illinois_100000_20190227\n",
      "Loading...\tdata/Texas_100000_20190227\n",
      "Loading...\tdata/California_100000_20190227\n",
      "Loading...\tdata/Georgia_100000_20190227\n",
      "Loading...\tdata/New York_100000_20190227\n"
     ]
    }
   ],
   "source": [
    "target_dir = 'data/'\n",
    "\n",
    "data_load = None\n",
    "for file_name in glob.glob(target_dir + '*'):\n",
    "    if not(re.search(r'\\.data$',file_name)):\n",
    "        print('Loading...\\t' + file_name)\n",
    "        if data_load is None:\n",
    "            data_load = pandas.read_pickle(file_name)\n",
    "        else:\n",
    "            data_load = pandas.concat([data_load,\n",
    "                                       pandas.read_pickle(file_name)], \n",
    "                                      sort=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>bmi</th>\n",
       "      <th>diastolic_blood_pressure</th>\n",
       "      <th>glucose</th>\n",
       "      <th>hdl_cholesterol</th>\n",
       "      <th>ldl_cholesterol</th>\n",
       "      <th>systolic_blood_pressure</th>\n",
       "      <th>total_cholesterol</th>\n",
       "      <th>triglycerides</th>\n",
       "      <th>age</th>\n",
       "      <th>framingham</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>319429.000000</td>\n",
       "      <td>319429.000000</td>\n",
       "      <td>319429.000000</td>\n",
       "      <td>319429.000000</td>\n",
       "      <td>319429.000000</td>\n",
       "      <td>319429.000000</td>\n",
       "      <td>319429.000000</td>\n",
       "      <td>319429.000000</td>\n",
       "      <td>319429.000000</td>\n",
       "      <td>314015.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>35.612186</td>\n",
       "      <td>88.651686</td>\n",
       "      <td>90.535694</td>\n",
       "      <td>62.771302</td>\n",
       "      <td>98.851847</td>\n",
       "      <td>137.660563</td>\n",
       "      <td>193.870900</td>\n",
       "      <td>157.440375</td>\n",
       "      <td>54.733059</td>\n",
       "      <td>10.577195</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>6.175320</td>\n",
       "      <td>13.516807</td>\n",
       "      <td>24.831421</td>\n",
       "      <td>14.734096</td>\n",
       "      <td>28.432366</td>\n",
       "      <td>26.829370</td>\n",
       "      <td>29.062353</td>\n",
       "      <td>82.369216</td>\n",
       "      <td>11.831822</td>\n",
       "      <td>5.426487</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>14.000000</td>\n",
       "      <td>66.800000</td>\n",
       "      <td>33.150000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>50.400000</td>\n",
       "      <td>96.700000</td>\n",
       "      <td>160.000000</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>2.433949</td>\n",
       "      <td>-6.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>31.600000</td>\n",
       "      <td>78.000000</td>\n",
       "      <td>74.700000</td>\n",
       "      <td>59.700000</td>\n",
       "      <td>78.700000</td>\n",
       "      <td>116.400000</td>\n",
       "      <td>173.200000</td>\n",
       "      <td>116.300000</td>\n",
       "      <td>46.053388</td>\n",
       "      <td>8.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>35.100000</td>\n",
       "      <td>83.600000</td>\n",
       "      <td>85.300000</td>\n",
       "      <td>66.200000</td>\n",
       "      <td>91.800000</td>\n",
       "      <td>127.900000</td>\n",
       "      <td>186.400000</td>\n",
       "      <td>132.300000</td>\n",
       "      <td>55.961670</td>\n",
       "      <td>12.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>39.000000</td>\n",
       "      <td>100.600000</td>\n",
       "      <td>96.100000</td>\n",
       "      <td>73.100000</td>\n",
       "      <td>109.500000</td>\n",
       "      <td>161.400000</td>\n",
       "      <td>199.400000</td>\n",
       "      <td>148.500000</td>\n",
       "      <td>64.221766</td>\n",
       "      <td>14.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>255.700000</td>\n",
       "      <td>123.500000</td>\n",
       "      <td>200.000000</td>\n",
       "      <td>80.000000</td>\n",
       "      <td>200.000000</td>\n",
       "      <td>203.300000</td>\n",
       "      <td>305.000000</td>\n",
       "      <td>599.900000</td>\n",
       "      <td>78.863792</td>\n",
       "      <td>26.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 bmi  diastolic_blood_pressure        glucose  \\\n",
       "count  319429.000000             319429.000000  319429.000000   \n",
       "mean       35.612186                 88.651686      90.535694   \n",
       "std         6.175320                 13.516807      24.831421   \n",
       "min        14.000000                 66.800000      33.150000   \n",
       "25%        31.600000                 78.000000      74.700000   \n",
       "50%        35.100000                 83.600000      85.300000   \n",
       "75%        39.000000                100.600000      96.100000   \n",
       "max       255.700000                123.500000     200.000000   \n",
       "\n",
       "       hdl_cholesterol  ldl_cholesterol  systolic_blood_pressure  \\\n",
       "count    319429.000000    319429.000000            319429.000000   \n",
       "mean         62.771302        98.851847               137.660563   \n",
       "std          14.734096        28.432366                26.829370   \n",
       "min           0.000000        50.400000                96.700000   \n",
       "25%          59.700000        78.700000               116.400000   \n",
       "50%          66.200000        91.800000               127.900000   \n",
       "75%          73.100000       109.500000               161.400000   \n",
       "max          80.000000       200.000000               203.300000   \n",
       "\n",
       "       total_cholesterol  triglycerides            age     framingham  \n",
       "count      319429.000000  319429.000000  319429.000000  314015.000000  \n",
       "mean          193.870900     157.440375      54.733059      10.577195  \n",
       "std            29.062353      82.369216      11.831822       5.426487  \n",
       "min           160.000000     100.000000       2.433949      -6.000000  \n",
       "25%           173.200000     116.300000      46.053388       8.000000  \n",
       "50%           186.400000     132.300000      55.961670      12.000000  \n",
       "75%           199.400000     148.500000      64.221766      14.000000  \n",
       "max           305.000000     599.900000      78.863792      26.000000  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_load.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Scale data and establish evaluation parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluation_samples = 100\n",
    "\n",
    "evaluation_features = [\n",
    "    'bmi',\n",
    "    'diastolic_blood_pressure',\n",
    "    'systolic_blood_pressure',\n",
    "    'glucose',\n",
    "    'hdl_cholesterol',\n",
    "    'ldl_cholesterol',\n",
    "    'total_cholesterol',\n",
    "    'triglycerides',\n",
    "    'age',\n",
    "    'framingham'    \n",
    "]\n",
    "\n",
    "data_feature_bounds = {\n",
    "    'bmi':(0,400),\n",
    "    'diastolic_blood_pressure':(60,140),\n",
    "    'systolic_blood_pressure':(90,250),\n",
    "    'glucose':(0,2000),\n",
    "    'hdl_cholesterol':(0,1500),\n",
    "    'ldl_cholesterol':(0,2000),\n",
    "    'total_cholesterol':(0,2100),\n",
    "    'triglycerides':(0,3000),\n",
    "    'age':(0,120),\n",
    "    'framingham':(-10,37)\n",
    "}\n",
    "target_feature_bounds = (0,1)\n",
    "\n",
    "# Setup for estimation of framingham score\n",
    "response = ['framingham']\n",
    "predictors = [ f for f in evaluation_features if f not in response]\n",
    "\n",
    "results_columns = [\n",
    "    'mechanism', \n",
    "    'query', \n",
    "    'iteration', \n",
    "    'metric', \n",
    "    'result', \n",
    "    'mechanism runtime (s)', \n",
    "    'total runtime (s)'\n",
    "]\n",
    "\n",
    "# Scale data 'data_feature_bounds' -> 'target_feature_bounds'\n",
    "data = scale_data(data_load[evaluation_features].dropna(),\n",
    "                  target_bounds=target_feature_bounds,\n",
    "                  data_bounds=data_feature_bounds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation of Sample Covariance Differential Privacy Methods\n",
    "\n",
    "## Gaussian Mechanism with symmetric and identity sampling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Symmetric Gaussian Mechanism\n",
    "\n",
    "Evaluation setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# labelling result values for mechanism\n",
    "mechanism = 'gaussian'\n",
    "query_type = 'covariance'\n",
    "metric = 'principle component RSS'\n",
    "\n",
    "result_pickle_location = 'results/'\n",
    "result_pickle_name = '_'.join([mechanism, \n",
    "                               query_type, \n",
    "                               str(evaluation_samples), \n",
    "                               datetime.date.today().strftime(\"%Y%m%d\")])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Differential Privacy parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epsilon = 1.0 \n",
    "# 1 / number of observations\n",
    "delta = pow(data.shape[0], -1)\n",
    "\n",
    "sensitivity = get_query_row_sensitivity(unit_of_change='tuple',\n",
    "                                        query_scale=target_feature_bounds,\n",
    "                                        data_shape=data.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evaluation of symmetric matrix gaussian mechanism sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = centered_sample_covariance_matrix(X=data)\n",
    "\n",
    "result = None\n",
    "sample = dict()\n",
    "\n",
    "for i in xrange(evaluation_samples): \n",
    "    # Sample mechanism\n",
    "    start_clock = datetime.datetime.now()\n",
    "    # Add symmetric iid noise\n",
    "    sample[i] = gaussian_mechanism_matrix_sample(\n",
    "                data=query,\n",
    "                epsilon=epsilon,\n",
    "                delta=delta,\n",
    "                sensitivity=sensitivity,\n",
    "                symmetric=True,\n",
    "                verbose=False)\n",
    "    end_sample_clock = datetime.datetime.now() \n",
    "\n",
    "    result = record_result(results=result, \n",
    "                           column_names=results_columns, \n",
    "                           new_data=[[mechanism, \n",
    "                                      query_type, \n",
    "                                      i+1,\n",
    "                                      metric, \n",
    "                                      principle_component_RSS(true=query, pred=sample[i]), \n",
    "                                      (end_sample_clock - start_clock).total_seconds(),\n",
    "                                      (end_sample_clock - start_clock).total_seconds()\n",
    "                                     ]])\n",
    "    \n",
    "result.to_pickle(result_pickle_location + result_pickle_name)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Identity Gaussian Mechanism\n",
    "\n",
    "Evaluation setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# labelling result values for mechanism\n",
    "mechanism = 'gaussian'\n",
    "query_type = 'itentity'\n",
    "metric = 'principle component RSS'\n",
    "\n",
    "result_pickle_name = '_'.join([mechanism, \n",
    "                               query_type, \n",
    "                               str(evaluation_samples), \n",
    "                               datetime.date.today().strftime(\"%Y%m%d\")])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Differential Privacy parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epsilon = 1.0 \n",
    "# 1 / number of observations\n",
    "delta = pow(data.shape[0], -1)\n",
    "\n",
    "sensitivity = get_query_row_sensitivity(unit_of_change='singleton',\n",
    "                                        query_scale=target_feature_bounds,\n",
    "                                        data_shape=data.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evaluation of identity query guassian mechanism sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = centered_sample_covariance_matrix(X=data)\n",
    "\n",
    "result = None\n",
    "\n",
    "for i in xrange(evaluation_samples): \n",
    "    # Sample mechanism\n",
    "    start_clock = datetime.datetime.now()\n",
    "    # Add symmetric iid noise\n",
    "    sample = gaussian_mechanism_matrix_sample(\n",
    "                data=query,\n",
    "                epsilon=epsilon,\n",
    "                delta=delta,\n",
    "                sensitivity=sensitivity,\n",
    "                symmetric=False,\n",
    "                verbose=False)\n",
    "    end_sample_clock = datetime.datetime.now() \n",
    "\n",
    "    sample_cov = centered_sample_covariance_matrix(X=sample)\n",
    "    end_loop_clock = datetime.datetime.now() \n",
    "    \n",
    "    result = record_result(results=result, \n",
    "                           column_names=results_columns, \n",
    "                           new_data=[[mechanism, \n",
    "                                      query_type, \n",
    "                                      i+1,\n",
    "                                      metric, \n",
    "                                      principle_component_RSS(true=query, pred=sample_cov), \n",
    "                                      (end_sample_clock - start_clock).total_seconds(),\n",
    "                                      (end_loop_clock - start_clock).total_seconds()\n",
    "                                     ]])\n",
    "    \n",
    "result.to_pickle(result_pickle_location + result_pickle_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  Evaluation of Data Release Differential Privacy Methods\n",
    "\n",
    "## Gaussian and Matrixvariate Gaussian Mechanisms by regression task\n",
    "\n",
    "### Identity Query "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Baseline\n",
    "\n",
    "Evaluation Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_obs, n_features = data.shape\n",
    "\n",
    "evaluation_test_split = 0.1\n",
    "evalaution_test_size = int(n_obs*evaluation_test_split)\n",
    "evaluation_train_size = n_obs - evalaution_test_size\n",
    "\n",
    "evaluation_samples = 2\n",
    "\n",
    "# labelling result values for mechanism\n",
    "mechanism = 'baseline'\n",
    "query_type = 'identity'\n",
    "metric = 'rmse'\n",
    "\n",
    "result_pickle_name = '_'.join([mechanism, \n",
    "                               query_type,\n",
    "                               ''.join(response), \n",
    "                               str(evaluation_samples), \n",
    "                               datetime.date.today().strftime(\"%Y%m%d\")])\n",
    "\n",
    "# Model parameters\n",
    "model_params = dict(epochs=5, batch_size=32, verbose=0) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evaluation of baseline model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = None\n",
    "\n",
    "for i in xrange(evaluation_samples):\n",
    "    start_clock = datetime.datetime.now()\n",
    "    # Train model and evaluate prediction metric on holdout set   \n",
    "    metric_result = seq_nn_single_evaluation(train_data=data,\n",
    "                                             test_data=data,\n",
    "                                             test_holdout_p=evaluation_test_split,\n",
    "                                             X_labels=predictors,\n",
    "                                             y_label=response,\n",
    "                                             fit_params=model_params)\n",
    "    \n",
    "    end_loop_clock = datetime.datetime.now() \n",
    "    \n",
    "    result = record_result(results=result, \n",
    "                           column_names=results_columns, \n",
    "                           new_data=[[mechanism, \n",
    "                                      query_type, \n",
    "                                      i+1,\n",
    "                                      metric, \n",
    "                                      metric_result, \n",
    "                                      0,\n",
    "                                      (end_loop_clock - start_clock).total_seconds()\n",
    "                                     ]])\n",
    "    \n",
    "result.to_pickle(result_pickle_location + result_pickle_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result.decsribe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Singleton Gaussian Mechanism\n",
    "\n",
    "Evaluation Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# labelling result values for mechanism\n",
    "mechanism = 'gaussian'\n",
    "query_type = 'identity'\n",
    "metric = 'rmse'\n",
    "\n",
    "result_pickle_name = '_'.join([mechanism, \n",
    "                               query_type, \n",
    "                               ''.join(response),\n",
    "                               str(evaluation_samples), \n",
    "                               datetime.date.today().strftime(\"%Y%m%d\")])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Differential Privacy Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "epsilon = 1.0 \n",
    "# 1 / number of observations\n",
    "delta = pow(evaluation_train_size, -1)\n",
    "\n",
    "sensitivity = get_query_point_sensitivity(unit_of_change='singleton',\n",
    "                                          query_scale=target_feature_bounds,\n",
    "                                          data_shape=(evaluation_train_size, n_features))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evaluation of singleton gaussian sequential NN model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Sample', <class 'pandas.core.frame.DataFrame'>, (282613, 10))\n",
      "Train on 282613 samples, validate on 31402 samples\n",
      "Epoch 1/10\n",
      "282613/282613 [==============================] - 7s 26us/step - loss: 0.0073 - mean_squared_error: 0.0072 - kullback_leibler_divergence: 0.0161 - mean_absolute_error: 0.0648 - val_loss: 0.0132 - val_mean_squared_error: 0.0132 - val_kullback_leibler_divergence: 0.0146 - val_mean_absolute_error: 0.0876\n",
      "Epoch 2/10\n",
      "282613/282613 [==============================] - 7s 23us/step - loss: 0.0060 - mean_squared_error: 0.0060 - kullback_leibler_divergence: 0.0071 - mean_absolute_error: 0.0619 - val_loss: 0.0134 - val_mean_squared_error: 0.0134 - val_kullback_leibler_divergence: 0.0297 - val_mean_absolute_error: 0.0902\n",
      "Epoch 3/10\n",
      "282613/282613 [==============================] - 6s 22us/step - loss: 0.0060 - mean_squared_error: 0.0060 - kullback_leibler_divergence: 0.0071 - mean_absolute_error: 0.0619 - val_loss: 0.0133 - val_mean_squared_error: 0.0133 - val_kullback_leibler_divergence: 0.0238 - val_mean_absolute_error: 0.0892\n",
      "Epoch 4/10\n",
      "282613/282613 [==============================] - 6s 22us/step - loss: 0.0060 - mean_squared_error: 0.0060 - kullback_leibler_divergence: 0.0071 - mean_absolute_error: 0.0619 - val_loss: 0.0133 - val_mean_squared_error: 0.0133 - val_kullback_leibler_divergence: 0.0262 - val_mean_absolute_error: 0.0896\n",
      "Epoch 5/10\n",
      "282613/282613 [==============================] - 6s 22us/step - loss: 0.0060 - mean_squared_error: 0.0060 - kullback_leibler_divergence: 0.0071 - mean_absolute_error: 0.0618 - val_loss: 0.0132 - val_mean_squared_error: 0.0132 - val_kullback_leibler_divergence: 0.0148 - val_mean_absolute_error: 0.0877\n",
      "Epoch 6/10\n",
      "282613/282613 [==============================] - 6s 22us/step - loss: 0.0060 - mean_squared_error: 0.0060 - kullback_leibler_divergence: 0.0071 - mean_absolute_error: 0.0618 - val_loss: 0.0133 - val_mean_squared_error: 0.0133 - val_kullback_leibler_divergence: 0.0239 - val_mean_absolute_error: 0.0892\n",
      "Epoch 7/10\n",
      "282613/282613 [==============================] - 6s 22us/step - loss: 0.0060 - mean_squared_error: 0.0060 - kullback_leibler_divergence: 0.0071 - mean_absolute_error: 0.0618 - val_loss: 0.0134 - val_mean_squared_error: 0.0134 - val_kullback_leibler_divergence: 0.0302 - val_mean_absolute_error: 0.0903\n",
      "Epoch 8/10\n",
      "282613/282613 [==============================] - 6s 21us/step - loss: 0.0060 - mean_squared_error: 0.0060 - kullback_leibler_divergence: 0.0071 - mean_absolute_error: 0.0618 - val_loss: 0.0133 - val_mean_squared_error: 0.0133 - val_kullback_leibler_divergence: 0.0226 - val_mean_absolute_error: 0.0890\n",
      "Epoch 9/10\n",
      "282613/282613 [==============================] - 6s 22us/step - loss: 0.0060 - mean_squared_error: 0.0060 - kullback_leibler_divergence: 0.0071 - mean_absolute_error: 0.0618 - val_loss: 0.0133 - val_mean_squared_error: 0.0133 - val_kullback_leibler_divergence: 0.0201 - val_mean_absolute_error: 0.0886\n",
      "Epoch 10/10\n",
      "282613/282613 [==============================] - 6s 22us/step - loss: 0.0060 - mean_squared_error: 0.0060 - kullback_leibler_divergence: 0.0071 - mean_absolute_error: 0.0618 - val_loss: 0.0133 - val_mean_squared_error: 0.0133 - val_kullback_leibler_divergence: 0.0196 - val_mean_absolute_error: 0.0886\n",
      "('Sample', <class 'pandas.core.frame.DataFrame'>, (282613, 10))\n",
      "Train on 282613 samples, validate on 31402 samples\n",
      "Epoch 1/10\n",
      "282613/282613 [==============================] - 7s 25us/step - loss: 0.0073 - mean_squared_error: 0.0072 - kullback_leibler_divergence: 0.0200 - mean_absolute_error: 0.0633 - val_loss: 0.0134 - val_mean_squared_error: 0.0134 - val_kullback_leibler_divergence: 0.0315 - val_mean_absolute_error: 0.0905\n",
      "Epoch 2/10\n",
      "282613/282613 [==============================] - 7s 23us/step - loss: 0.0055 - mean_squared_error: 0.0055 - kullback_leibler_divergence: 0.0067 - mean_absolute_error: 0.0592 - val_loss: 0.0133 - val_mean_squared_error: 0.0133 - val_kullback_leibler_divergence: 0.0294 - val_mean_absolute_error: 0.0899\n",
      "Epoch 3/10\n",
      "282613/282613 [==============================] - 7s 25us/step - loss: 0.0055 - mean_squared_error: 0.0055 - kullback_leibler_divergence: 0.0067 - mean_absolute_error: 0.0592 - val_loss: 0.0137 - val_mean_squared_error: 0.0137 - val_kullback_leibler_divergence: 0.0398 - val_mean_absolute_error: 0.0931\n",
      "Epoch 4/10\n",
      "282613/282613 [==============================] - 7s 25us/step - loss: 0.0055 - mean_squared_error: 0.0055 - kullback_leibler_divergence: 0.0067 - mean_absolute_error: 0.0592 - val_loss: 0.0136 - val_mean_squared_error: 0.0136 - val_kullback_leibler_divergence: 0.0370 - val_mean_absolute_error: 0.0923\n",
      "Epoch 5/10\n",
      "282613/282613 [==============================] - 7s 24us/step - loss: 0.0055 - mean_squared_error: 0.0055 - kullback_leibler_divergence: 0.0067 - mean_absolute_error: 0.0592 - val_loss: 0.0133 - val_mean_squared_error: 0.0133 - val_kullback_leibler_divergence: 0.0286 - val_mean_absolute_error: 0.0898\n",
      "Epoch 6/10\n",
      "282613/282613 [==============================] - 7s 23us/step - loss: 0.0055 - mean_squared_error: 0.0055 - kullback_leibler_divergence: 0.0067 - mean_absolute_error: 0.0591 - val_loss: 0.0135 - val_mean_squared_error: 0.0135 - val_kullback_leibler_divergence: 0.0337 - val_mean_absolute_error: 0.0912\n",
      "Epoch 7/10\n",
      "282613/282613 [==============================] - 7s 23us/step - loss: 0.0055 - mean_squared_error: 0.0055 - kullback_leibler_divergence: 0.0067 - mean_absolute_error: 0.0591 - val_loss: 0.0136 - val_mean_squared_error: 0.0136 - val_kullback_leibler_divergence: 0.0369 - val_mean_absolute_error: 0.0922\n",
      "Epoch 8/10\n",
      "282613/282613 [==============================] - 7s 24us/step - loss: 0.0055 - mean_squared_error: 0.0055 - kullback_leibler_divergence: 0.0067 - mean_absolute_error: 0.0591 - val_loss: 0.0134 - val_mean_squared_error: 0.0134 - val_kullback_leibler_divergence: 0.0344 - val_mean_absolute_error: 0.0914\n",
      "Epoch 9/10\n",
      "282613/282613 [==============================] - 7s 24us/step - loss: 0.0055 - mean_squared_error: 0.0055 - kullback_leibler_divergence: 0.0066 - mean_absolute_error: 0.0591 - val_loss: 0.0138 - val_mean_squared_error: 0.0138 - val_kullback_leibler_divergence: 0.0418 - val_mean_absolute_error: 0.0937\n",
      "Epoch 10/10\n",
      "282613/282613 [==============================] - 7s 24us/step - loss: 0.0055 - mean_squared_error: 0.0055 - kullback_leibler_divergence: 0.0067 - mean_absolute_error: 0.0591 - val_loss: 0.0135 - val_mean_squared_error: 0.0135 - val_kullback_leibler_divergence: 0.0351 - val_mean_absolute_error: 0.0916\n"
     ]
    }
   ],
   "source": [
    "result = None\n",
    "\n",
    "for i in xrange(evaluation_samples):\n",
    "    \n",
    "    start_clock = datetime.datetime.now()\n",
    "    \n",
    "    train_ind, test_ind = \\\n",
    "            test_train_split(len(data),\n",
    "                             evaluation_test_split)\n",
    "        \n",
    "    sample = gaussian_mechanism_matrix_sample(\n",
    "        data=data.iloc[train_ind],\n",
    "        epsilon=epsilon,\n",
    "        delta=delta,\n",
    "        sensitivity=sensitivity,\n",
    "        symmetric=False,\n",
    "        verbose=False)\n",
    "    \n",
    "    end_sample_clock = datetime.datetime.now() \n",
    "        \n",
    "    # Train model and evaluate prediction metric on holdout set   \n",
    "    metric_result = seq_nn_single_evaluation(train_data=sample,\n",
    "                                             test_data=data,\n",
    "                                             X_labels=predictors,\n",
    "                                             y_label=response,\n",
    "                                             train_ind=train_ind, \n",
    "                                             test_ind=test_ind,\n",
    "                                             fit_params=model_params)\n",
    "    \n",
    "    end_loop_clock = datetime.datetime.now() \n",
    "    \n",
    "    result = record_result(results=result, \n",
    "                           column_names=results_columns, \n",
    "                           new_data=[[mechanism, \n",
    "                                      query_type, \n",
    "                                      i+1,\n",
    "                                      metric, \n",
    "                                      metric_result, \n",
    "                                      (end_sample_clock - start_clock).total_seconds(),\n",
    "                                      (end_loop_clock - start_clock).total_seconds()\n",
    "                                     ]])\n",
    "    \n",
    "result.to_pickle(result_pickle_location + result_pickle_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>iteration</th>\n",
       "      <th>result</th>\n",
       "      <th>mechanism runtime (s)</th>\n",
       "      <th>total runtime (s)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>2.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>2.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>1.500000</td>\n",
       "      <td>0.115596</td>\n",
       "      <td>24.722100</td>\n",
       "      <td>91.192340</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>0.707107</td>\n",
       "      <td>0.000685</td>\n",
       "      <td>0.152010</td>\n",
       "      <td>3.018122</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.115111</td>\n",
       "      <td>24.614613</td>\n",
       "      <td>89.058205</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>1.250000</td>\n",
       "      <td>0.115353</td>\n",
       "      <td>24.668357</td>\n",
       "      <td>90.125272</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>1.500000</td>\n",
       "      <td>0.115596</td>\n",
       "      <td>24.722100</td>\n",
       "      <td>91.192340</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>1.750000</td>\n",
       "      <td>0.115838</td>\n",
       "      <td>24.775844</td>\n",
       "      <td>92.259407</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>2.000000</td>\n",
       "      <td>0.116080</td>\n",
       "      <td>24.829588</td>\n",
       "      <td>93.326474</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       iteration    result  mechanism runtime (s)  total runtime (s)\n",
       "count   2.000000  2.000000               2.000000           2.000000\n",
       "mean    1.500000  0.115596              24.722100          91.192340\n",
       "std     0.707107  0.000685               0.152010           3.018122\n",
       "min     1.000000  0.115111              24.614613          89.058205\n",
       "25%     1.250000  0.115353              24.668357          90.125272\n",
       "50%     1.500000  0.115596              24.722100          91.192340\n",
       "75%     1.750000  0.115838              24.775844          92.259407\n",
       "max     2.000000  0.116080              24.829588          93.326474"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Matrix-variate Gaussian Mechanism\n",
    "\n",
    "Binary Allocation Strategy - Key features\n",
    "   \n",
    "    key features = ['age','total_cholesterol','framingham'] \n",
    "    \n",
    "    'age' and 'cholesterol' important as contribute the largest scores to the total. \n",
    "    'framingham' important as the target variable.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evaluation Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MVG_identity_framingham_100_20190227\n"
     ]
    }
   ],
   "source": [
    "# labelling result values for mechanism\n",
    "mechanism = 'MVG_binary_knowledge'\n",
    "query_type = 'identity'\n",
    "metric = 'rmse'\n",
    "\n",
    "result_pickle_name = '_'.join([mechanism, \n",
    "                               query_type, \n",
    "                               ''.join(response),\n",
    "                               str(evaluation_samples), \n",
    "                               datetime.date.today().strftime(\"%Y%m%d\")])\n",
    "\n",
    "print(result_pickle_name)\n",
    "\n",
    "model_params = dict(epochs=10, batch_size=16, verbose=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Differential Privacy Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "sensitivity = get_query_row_sensitivity(query_type='identity',\n",
    "                                        query_scale=target_feature_bounds,\n",
    "                                        query_shape=(evaluation_train_size, n_features))\n",
    "\n",
    "gamma = get_query_gamma(query_scale=target_feature_bounds, \n",
    "                        query_shape=(evaluation_train_size, n_features), \n",
    "                        query_type='identity')\n",
    "\n",
    "# Allocation percentages in 'key_features_allocation' to key features \n",
    "# and remainder to all other features\n",
    "key_features_binary_mvg = ['age','total_cholesterol','framingham']  \n",
    "key_features_allocation = [0.45,0.55,0.65,0.75,0.85,0.95]\n",
    "\n",
    "feature_allocations = dict()\n",
    "for allocation in key_features_allocation:\n",
    "    \n",
    "    feature_allocations[allocation] = [ \n",
    "        allocation / len(key_features_binary_mvg)\n",
    "        if feature in key_features_binary_mvg \n",
    "        else (1 - allocation) / (n_features - len(key_features_binary_mvg))\n",
    "        for feature in evaluation_features \n",
    "    ]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Matrix-variate Gaussian Mechnaism Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = None \n",
    "for allocation in key_features_allocation:\n",
    "    \n",
    "    params = dict(\n",
    "        epsilon=epsilon,\n",
    "        delta=delta,\n",
    "        sensitivity=sensitivity,\n",
    "        gamma=gamma,\n",
    "        precision_allocation=feature_allocations[allocation],\n",
    "        precision_direction=numpy.identity(features),\n",
    "        covariance_direction='unimodal features',\n",
    "        covariance_method='binary'\n",
    "    )\n",
    "        \n",
    "    for i in xrange(evaluation_samples):\n",
    "        start_clock = datetime.datetime.now() \n",
    "        train_ind, test_ind = \\\n",
    "            test_train_split(len(data),\n",
    "                             evaluation_test_split)\n",
    "            \n",
    "        sample = matrixvariate_gaussian_mechanism_sample(data=data.iloc[train_ind],\n",
    "                                                         **params)\n",
    "        \n",
    "        end_sample_clock = datetime.datetime.now() \n",
    "        \n",
    "        metric_result = seq_nn_cross_validation(train_data=sample,\n",
    "                                                test_data=data,\n",
    "                                                X_labels=predictors,\n",
    "                                                y_label=response,\n",
    "                                                train_ind=train_ind, \n",
    "                                                test_ind=test_ind,\n",
    "                                                fit_params=model_params)\n",
    "        \n",
    "        end_loop_clock = datetime.datetime.now() \n",
    "        \n",
    "        result = record_result(results=result, \n",
    "                               column_names=results_columns, \n",
    "                               new_data=[[mechanism, \n",
    "                                          query_type, \n",
    "                                          i+1,\n",
    "                                          metric, \n",
    "                                          metric_result, \n",
    "                                          (end_sample_clock - start_clock).total_seconds(),\n",
    "                                          (end_loop_clock - start_clock).total_seconds()\n",
    "                                         ]])\n",
    "        \n",
    "            "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Binary Allocation Strategy - Key features\n",
    "   \n",
    "    Features allocations are proprotional to the singular values or explained directional variance\n",
    "    \n",
    "    Directions are equal to eigenvectors of the sample covariance. \n",
    "    These are the orthogonal primary axis of the variation in the sample covariance "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "from model_evaluation import cross_validation_split\n",
    "from model_evaluation import test_train_split\n",
    "from model_evaluation import root_mean_squared_error\n",
    "from tensorflow.keras import metrics\n",
    "from tensorflow.keras import layers\n",
    "import tensorflow as tf\n",
    "\n",
    "# Remove below if with to use GPU\n",
    "import os\n",
    "os.environ[\"CUDA_DEVICE_ORDER\"] = \"PCI_BUS_ID\"   # see issue #152\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = '-1'\n",
    "\n",
    "\n",
    "def keras_seq_reg_model_compilation(features, tf_loss, tf_metrics):\n",
    "\n",
    "    model_layers = [\n",
    "        layers.Dense(features, activation='relu', kernel_initializer='normal',\n",
    "                     kernel_regularizer=tf.keras.regularizers.l2(0.01)),\n",
    "\n",
    "        layers.Dense(int(features/2), activation='relu',\n",
    "                     kernel_initializer='normal'),\n",
    "\n",
    "        layers.Dense(1, kernel_initializer='normal')\n",
    "    ]\n",
    "\n",
    "    model = tf.keras.Sequential(model_layers)\n",
    "\n",
    "    model.compile(optimizer=tf.train.AdamOptimizer(0.001),\n",
    "                  loss=tf_loss,\n",
    "                  metrics=tf_metrics)\n",
    "\n",
    "    return model\n",
    "\n",
    "\n",
    "def seq_nn_cross_validation(train_data,\n",
    "                            test_data,\n",
    "                            folds,\n",
    "                            X_labels,\n",
    "                            y_label,\n",
    "                            fit_params):\n",
    "\n",
    "    result = list()\n",
    "    for train_ind, test_ind in cross_validation_split(len(train_data),\n",
    "                                                      folds=folds):\n",
    "\n",
    "        train = train_data.iloc[train_ind]\n",
    "        X_train = train[X_labels].values\n",
    "        y_train = train[y_label].values\n",
    "\n",
    "        test = test_data.iloc[test_ind]\n",
    "        X_test = test[X_labels].values\n",
    "        y_test = test[y_label].values\n",
    "\n",
    "        # Create sequential NN model\n",
    "        model = keras_seq_reg_model_compilation(\n",
    "            features=X_test.shape[1],\n",
    "            tf_loss=tf.losses.mean_squared_error,\n",
    "            tf_metrics=[metrics.mean_squared_error,\n",
    "                        metrics.kullback_leibler_divergence,\n",
    "                        metrics.mean_absolute_error])\n",
    "\n",
    "        # Train model\n",
    "        model.fit(X_train,\n",
    "                  y_train,\n",
    "#                   validation_data=(X_test, y_test),\n",
    "                  **fit_params)\n",
    "\n",
    "        if 'batch_size' not in fit_params.keys():\n",
    "            fit_params['batch_size'] = 32\n",
    "\n",
    "        result += [root_mean_squared_error(\n",
    "            model.predict(X_test, batch_size=fit_params['batch_size']),\n",
    "            y_test)]\n",
    "\n",
    "    return result\n",
    "\n",
    "\n",
    "def seq_nn_single_evaluation(train_data,\n",
    "                             test_data,\n",
    "                             X_labels,\n",
    "                             y_label,\n",
    "                             fit_params,\n",
    "                             test_holdout_p=None,\n",
    "                             train_ind=None,\n",
    "                             test_ind=None):\n",
    "\n",
    "    if train_ind is None or test_ind is None:\n",
    "        train_ind, test_ind = \\\n",
    "            test_train_split(len(test_data),\n",
    "                             test_holdout_p)\n",
    "\n",
    "        train = train_data.iloc[train_ind]\n",
    "        X_train = train[X_labels].values\n",
    "        y_train = train[y_label].values\n",
    "    else:\n",
    "        X_train = train_data[X_labels].values\n",
    "        y_train = train_data[y_label].values\n",
    "\n",
    "    test = test_data.iloc[test_ind]\n",
    "    X_test = test[X_labels].values\n",
    "    y_test = test[y_label].values\n",
    "\n",
    "    # Create sequential NN model\n",
    "    model = keras_seq_reg_model_compilation(\n",
    "        features=X_test.shape[1],\n",
    "        tf_loss=tf.losses.mean_squared_error,\n",
    "        tf_metrics=[metrics.mean_squared_error,\n",
    "                    metrics.kullback_leibler_divergence,\n",
    "                    metrics.mean_absolute_error])\n",
    "\n",
    "    # Train model\n",
    "    model.fit(X_train,\n",
    "              y_train,\n",
    "#               validation_data=(X_test, y_test),\n",
    "              **fit_params)\n",
    "\n",
    "    if 'batch_size' not in fit_params.keys():\n",
    "        fit_params['batch_size'] = 32\n",
    "\n",
    "    result = root_mean_squared_error(\n",
    "        model.predict(X_test, batch_size=fit_params['batch_size']),\n",
    "        y_test)\n",
    "\n",
    "    return result\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy\n",
    "\n",
    "# Establish parameters: gaussian_sensitivity, MVG_sensitivity\n",
    "\n",
    "\n",
    "def centered_covariance_query_sensitivity(n, m, c):\n",
    "    '''\n",
    "       'A Differential Privacy Mechanism Design Under Matrix-Valued Query'\n",
    "        Chanyaswad, Dytso, Poor & Mittal 2018, p.18.:\n",
    "        https://arxiv.org/abs/1802.10077 (accessed 16/12/2018) [1]\n",
    "\n",
    "       Requires\n",
    "           n   - divisor of query function\n",
    "           m   - number of unit values / observations to be varied\n",
    "                 under adjacency definition\n",
    "           c   - maximum possible value in range of single observation\n",
    "\n",
    "       Returns - Sensitivity calculation for zero mean\n",
    "                 covariance estimation query\n",
    "                 ie f(X) = n^-1 * transpose(X)X\n",
    "    '''\n",
    "    return (2 * float(m) * float(c)**2) / float(n)\n",
    "\n",
    "\n",
    "'''\n",
    "    Examples:\n",
    "        get_query_point_sensitivity(query_type='identity',\n",
    "                                    query_scale=feature_scale,\n",
    "                                    query_shape=(train_size,features)\n",
    "        get_query_point_sensitivity(query_type='covariance',\n",
    "                                    query_scale=feature_scale,\n",
    "                                    query_shape=(train_size,features)\n",
    "'''\n",
    "\n",
    "\n",
    "def get_query_point_sensitivity(query_scale, query_shape, query_type):\n",
    "    if query_type == 'identity':\n",
    "        # Global maximum of change for any\n",
    "        # single point change in query f(X) = X\n",
    "        result = abs(numpy.subtract(query_scale[0], query_scale[1]))\n",
    "    elif query_type == 'covariance':\n",
    "        # Global maximum of change for a single point\n",
    "        # change in query f(X) = (1/n)*transpose(X)X\n",
    "        result = centered_covariance_query_sensitivity(n=query_shape[0],\n",
    "                                                       m=1.0,\n",
    "                                                       c=query_scale[1])\n",
    "    else:\n",
    "        print(\"get_query_point_sensitivity: \\\n",
    "        required query_type in ('identity','covariance')\")\n",
    "        result = None\n",
    "    return result\n",
    "\n",
    "\n",
    "'''\n",
    "    Examples:\n",
    "        get_query_row_sensitivity(query_type='identity',\n",
    "                                  query_scale=feature_scale,\n",
    "                                  query_shape=(train_size,features))\n",
    "        get_query_row_sensitivity(query_type='covariance',\n",
    "                                  query_scale=feature_scale,\n",
    "                                  query_shape=(train_size,features))\n",
    "'''\n",
    "\n",
    "\n",
    "def get_query_row_sensitivity(query_scale, query_shape, query_type):\n",
    "    if query_type == 'identity':\n",
    "        # Global maximum of change for any data row / observation\n",
    "        # change in query f(X) = X\n",
    "        # Section 8.1.3 p30 of paper [1]\n",
    "        sensitivity = get_query_point_sensitivity(query_scale,\n",
    "                                                  query_shape,\n",
    "                                                  'identity')\n",
    "        result = pow(query_shape[1] * pow(sensitivity, 2),\n",
    "                     0.5)\n",
    "    elif query_type == 'covariance':\n",
    "        # Global maximum of change for any data\n",
    "        # row / observation change in query f(X) = n^-1 * transpose(X)X\n",
    "        result = centered_covariance_query_sensitivity(n=query_shape[0],\n",
    "                                                       m=query_shape[1],\n",
    "                                                       c=query_scale[1])\n",
    "    else:\n",
    "        print(\"get_query_row_sensitivity: \\\n",
    "        required query_type in ('identity','covariance')\")\n",
    "        result = None\n",
    "    return result\n",
    "\n",
    "\n",
    "'''\n",
    "    Examples:\n",
    "        get_query_gamma(feature_scale,\n",
    "                        (train_size,features),\n",
    "                        'identity')\n",
    "        get_query_gamma(feature_scale,\n",
    "                        (features,features),\n",
    "                        query_type='covariance'))\n",
    "'''\n",
    "\n",
    "\n",
    "def get_query_gamma(query_scale, query_shape, query_type):\n",
    "    obs, features = query_shape\n",
    "    if query_type == 'identity':\n",
    "        # Defined as sup X ||f(X)||F, where ||f(X)||F is the\n",
    "        # Frobenious norm of the query f(X)\n",
    "        result = pow(obs * features * query_scale[1], 0.5)\n",
    "    elif query_type == 'covariance':\n",
    "        # Defined as m * c^2 Section 4.4 example 1, p17 in [1]\n",
    "        result = features*pow(query_scale[1], 2)\n",
    "    else:\n",
    "        print(\"get_query_gamma: \\\n",
    "        required query_type in ('identity','covariance')\")\n",
    "        result = None\n",
    "    return result\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
